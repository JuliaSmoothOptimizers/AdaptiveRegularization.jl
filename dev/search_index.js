var documenterSearchIndex = {"docs":
[{"location":"91-developer/#dev_docs","page":"Developer documentation","title":"Developer documentation","text":"","category":"section"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"note: Contributing guidelines\nIf you haven't, please read the Contributing guidelines first.","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"If you want to make contributions to this package that involves code, then this guide is for you.","category":"page"},{"location":"91-developer/#First-time-clone","page":"Developer documentation","title":"First time clone","text":"","category":"section"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"tip: If you have writing rights\nIf you have writing rights, you don't have to fork. Instead, simply clone and skip ahead. Whenever upstream is mentioned, use origin instead.","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"If this is the first time you work with this repository, follow the instructions below to clone the repository.","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"Fork this repo\nClone your repo (this will create a git remote called origin)\nAdd this repo as a remote:\ngit remote add upstream https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"This will ensure that you have two remotes in your git: origin and upstream. You will create branches and push to origin, and you will fetch and update your local main branch from upstream.","category":"page"},{"location":"91-developer/#Linting-and-formatting","page":"Developer documentation","title":"Linting and formatting","text":"","category":"section"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"Install a plugin on your editor to use EditorConfig. This will ensure that your editor is configured with important formatting settings.","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"We use https://pre-commit.com to run the linters and formatters. In particular, the Julia code is formatted using JuliaFormatter.jl, so please install it globally first:","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"julia> # Press ]\npkg> activate\npkg> add JuliaFormatter","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"To install pre-commit, we recommend using pipx as follows:","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"# Install pipx following the link\npipx install pre-commit","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"With pre-commit installed, activate it as a pre-commit hook:","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"pre-commit install","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"To run the linting and formatting manually, enter the command below:","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"pre-commit run -a","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"Now, you can only commit if all the pre-commit tests pass.","category":"page"},{"location":"91-developer/#Testing","page":"Developer documentation","title":"Testing","text":"","category":"section"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"As with most Julia packages, you can just open Julia in the repository folder, activate the environment, and run test:","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"julia> # press ]\npkg> activate .\npkg> test","category":"page"},{"location":"91-developer/#Working-on-a-new-issue","page":"Developer documentation","title":"Working on a new issue","text":"","category":"section"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"We try to keep a linear history in this repo, so it is important to keep your branches up-to-date.","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"Fetch from the remote and fast-forward your local main\ngit fetch upstream\ngit switch main\ngit merge --ff-only upstream/main\nBranch from main to address the issue (see below for naming)\ngit switch -c 42-add-answer-universe\nPush the new local branch to your personal remote repository\ngit push -u origin 42-add-answer-universe\nCreate a pull request to merge your remote branch into the org main.","category":"page"},{"location":"91-developer/#Branch-naming","page":"Developer documentation","title":"Branch naming","text":"","category":"section"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"If there is an associated issue, add the issue number.\nIf there is no associated issue, and the changes are small, add a prefix such as \"typo\", \"hotfix\", \"small-refactor\", according to the type of update.\nIf the changes are not small and there is no associated issue, then create the issue first, so we can properly discuss the changes.\nUse dash separated imperative wording related to the issue (e.g., 14-add-tests, 15-fix-model, 16-remove-obsolete-files).","category":"page"},{"location":"91-developer/#Commit-message","page":"Developer documentation","title":"Commit message","text":"","category":"section"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"Use imperative or present tense, for instance: Add feature or Fix bug.\nHave informative titles.\nWhen necessary, add a body with details.\nIf there are breaking changes, add the information to the commit message.","category":"page"},{"location":"91-developer/#Before-creating-a-pull-request","page":"Developer documentation","title":"Before creating a pull request","text":"","category":"section"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"tip: Atomic git commits\nTry to create \"atomic git commits\" (recommended reading: The Utopic Git History).","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"Make sure the tests pass.\nMake sure the pre-commit tests pass.\nFetch any main updates from upstream and rebase your branch, if necessary:\ngit fetch upstream\ngit rebase upstream/main BRANCH_NAME\nThen you can open a pull request and work with the reviewer to address any issues.","category":"page"},{"location":"91-developer/#Building-and-viewing-the-documentation-locally","page":"Developer documentation","title":"Building and viewing the documentation locally","text":"","category":"section"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"Following the latest suggestions, we recommend using LiveServer to build the documentation. Here is how you do it:","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"Run julia --project=docs to open Julia in the environment of the docs.\nIf this is the first time building the docs\nPress ] to enter pkg mode\nRun pkg> dev . to use the development version of your package\nPress backspace to leave pkg mode\nRun julia> using LiveServer\nRun julia> servedocs()","category":"page"},{"location":"91-developer/#Making-a-new-release","page":"Developer documentation","title":"Making a new release","text":"","category":"section"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"To create a new release, you can follow these simple steps:","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"Create a branch release-x.y.z\nUpdate version in Project.toml\nUpdate the CHANGELOG.md:\nRename the section \"Unreleased\" to \"[x.y.z] - yyyy-mm-dd\" (i.e., version under brackets, dash, and date in ISO format)\nAdd a new section on top of it named \"Unreleased\"\nAdd a new link in the bottom for version \"x.y.z\"\nChange the \"[unreleased]\" link to use the latest version - end of line, vx.y.z ... HEAD.\nCreate a commit \"Release vx.y.z\", push, create a PR, wait for it to pass, merge the PR.\nGo back to main screen and click on the latest commit (link: https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/commit/main)\nAt the bottom, write @JuliaRegistrator register","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"After that, you only need to wait and verify:","category":"page"},{"location":"91-developer/","page":"Developer documentation","title":"Developer documentation","text":"Wait for the bot to comment (should take < 1m) with a link to a PR to the registry\nFollow the link and wait for a comment on the auto-merge\nThe comment should said all is well and auto-merge should occur shortly\nAfter the merge happens, TagBot will trigger and create a new GitHub tag. Check on https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/releases\nAfter the release is create, a \"docs\" GitHub action will start for the tag.\nAfter it passes, a deploy action will run.\nAfter that runs, the stable docs should be updated. Check them and look for the version number.","category":"page"},{"location":"95-reference/#reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"95-reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"95-reference/","page":"Reference","title":"Reference","text":"Pages = [\"95-reference.md\"]","category":"page"},{"location":"95-reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"95-reference/","page":"Reference","title":"Reference","text":"Pages = [\"95-reference.md\"]","category":"page"},{"location":"95-reference/","page":"Reference","title":"Reference","text":"Modules = [AdaptiveRegularization]","category":"page"},{"location":"95-reference/#AdaptiveRegularization.HessDense","page":"Reference","title":"AdaptiveRegularization.HessDense","text":"HessDense(::AbstractNLPModel{T,S}, n)\n\nReturn a structure used for the evaluation of dense Hessian matrix.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#AdaptiveRegularization.HessGaussNewtonOp","page":"Reference","title":"AdaptiveRegularization.HessGaussNewtonOp","text":"HessGaussNewtonOp(::AbstractNLSModel{T,S}, n)\n\nReturn a structure used for the evaluation of the Hessian matrix as an operator.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#AdaptiveRegularization.HessOp","page":"Reference","title":"AdaptiveRegularization.HessOp","text":"HessOp(::AbstractNLPModel{T,S}, n)\n\nReturn a structure used for the evaluation of the Hessian matrix as an operator.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#AdaptiveRegularization.HessSparse","page":"Reference","title":"AdaptiveRegularization.HessSparse","text":"HessSparse(::AbstractNLPModel{T,S}, n)\n\nReturn a structure used for the evaluation of sparse Hessian matrix.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#AdaptiveRegularization.HessSparseCOO","page":"Reference","title":"AdaptiveRegularization.HessSparseCOO","text":"HessSparseCOO(::AbstractNLPModel{T,S}, n)\n\nReturn a structure used for the evaluation of sparse Hessian matrix in COO-format.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#AdaptiveRegularization.PDataKARC","page":"Reference","title":"AdaptiveRegularization.PDataKARC","text":"PDataKARC(::Type{S}, ::Type{T}, n)\n\nReturn a structure used for the preprocessing of ARCqK methods.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#AdaptiveRegularization.PDataNLSST","page":"Reference","title":"AdaptiveRegularization.PDataNLSST","text":"PDataNLSST(::Type{S}, ::Type{T}, n)\n\nReturn a structure used for the preprocessing of Steihaug-Toint methods for Gauss-Newton approximation of nonlinear least squares.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#AdaptiveRegularization.PDataST","page":"Reference","title":"AdaptiveRegularization.PDataST","text":"PDataST(::Type{S}, ::Type{T}, n)\n\nReturn a structure used for the preprocessing of Steihaug-Toint methods.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#AdaptiveRegularization.PDataTRK","page":"Reference","title":"AdaptiveRegularization.PDataTRK","text":"PDataTRK(::Type{S}, ::Type{T}, n)\n\nReturn a structure used for the preprocessing of TRK methods.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#AdaptiveRegularization.TRARCSolver","page":"Reference","title":"AdaptiveRegularization.TRARCSolver","text":"TRARCSolver(nlp::AbstractNLPModel [, x0 = nlp.meta.x0]; kwargs...)\nTRARCSolver(stp::NLPStopping; kwargs...)\n\nStructure regrouping all the structure used during the TRARC call. It returns a TRARCSolver structure.\n\nArguments\n\nThe keyword arguments may include:\n\nstp::NLPStopping: Stopping structure for this algorithm workflow;\nmeta::ParamData: see ParamData;\nworkspace::TRARCWorkspace: allocated space for the solver itself;\nTR::ARTrustRegion: trust-region parameters.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#AdaptiveRegularization.TRARCWorkspace","page":"Reference","title":"AdaptiveRegularization.TRARCWorkspace","text":"TRARCWorkspace(nlp, ::Type{Hess}, n)\n\nPre-allocate the memory used during the TRARC call for the problem nlp of size n. The possible values for Hess are: HessDense, HessSparse, HessSparseCOO, HessOp. Return a TRARCWorkspace structure.\n\n\n\n\n\n","category":"type"},{"location":"95-reference/#AdaptiveRegularization.ARCqKCOO-Tuple{Stopping.NLPStopping}","page":"Reference","title":"AdaptiveRegularization.ARCqKCOO","text":"TRARC(nlp; kwargs...)\n\nCompute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.\n\nSome variants of TRARC are already implemented and listed in AdaptiveRegularization.ALL_solvers.\n\nArguments\n\nnlp::AbstractNLPModel: the model solved, see NLPModels.jl.\n\nThe keyword arguments include\n\nTR::ARTrustRegion: structure with trust-region/ARC parameters, see SolverTools.jl. Default: ARTrustRegion(T(10.0)).\nhess_type::Type{Hess}: Structure used to handle the hessian. The possible values are: HessDense, HessSparse, HessSparseCOO, HessOp. Default: HessOp.\npdata_type::Type{ParamData} Structure used for the preprocessing step. Default: PDataKARC.\nrobust::Bool: true implements a robust evaluation of the model. Default: true.\nverbose::Bool: true prints iteration information. Default: false.\n\nAdditional kwargs are used for stopping criterion, see Stopping.jl.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.stp: stopping object used for the algorithm;\nsolver.workspace: additional allocations;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nThis implementation uses Stopping.jl. Therefore, it is also possible to used\n\nTRARC(stp; kwargs...)\n\nwhich returns the stp::NLPStopping updated.\n\nFor advanced usage, first define a TRARCSolver to preallocate the  memory used in the algorithm, and then call solve!:\n\nstats = solve!(solver, nlp)\nstats = solve!(solver, nlp, stats)\n\nTo choose a particular variant, the keyword arguments hess_type and pdata_type can be used as follows:    TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC) the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.\n\nReferences\n\nThis method unifies the implementation of trust-region and adaptive regularization with cubics as described in\n\nDussault, J.-P. (2020).\nA unified efficient implementation of trust-region type algorithms for unconstrained optimization.\nINFOR: Information Systems and Operational Research, 58(2), 290-309.\n10.1080/03155986.2019.1624490\n\nExamples\n\nusing AdaptiveRegularization, ADNLPModels\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nstats = TRARC(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = solve!(solver, nlp)\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = GenericExecutionStats(nlp)\nstats = solve!(solver, nlp, stats)\n\n\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.ARCqKOp-Tuple{Stopping.NLPStopping}","page":"Reference","title":"AdaptiveRegularization.ARCqKOp","text":"TRARC(nlp; kwargs...)\n\nCompute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.\n\nSome variants of TRARC are already implemented and listed in AdaptiveRegularization.ALL_solvers.\n\nArguments\n\nnlp::AbstractNLPModel: the model solved, see NLPModels.jl.\n\nThe keyword arguments include\n\nTR::ARTrustRegion: structure with trust-region/ARC parameters, see SolverTools.jl. Default: ARTrustRegion(T(10.0)).\nhess_type::Type{Hess}: Structure used to handle the hessian. The possible values are: HessDense, HessSparse, HessSparseCOO, HessOp. Default: HessOp.\npdata_type::Type{ParamData} Structure used for the preprocessing step. Default: PDataKARC.\nrobust::Bool: true implements a robust evaluation of the model. Default: true.\nverbose::Bool: true prints iteration information. Default: false.\n\nAdditional kwargs are used for stopping criterion, see Stopping.jl.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.stp: stopping object used for the algorithm;\nsolver.workspace: additional allocations;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nThis implementation uses Stopping.jl. Therefore, it is also possible to used\n\nTRARC(stp; kwargs...)\n\nwhich returns the stp::NLPStopping updated.\n\nFor advanced usage, first define a TRARCSolver to preallocate the  memory used in the algorithm, and then call solve!:\n\nstats = solve!(solver, nlp)\nstats = solve!(solver, nlp, stats)\n\nTo choose a particular variant, the keyword arguments hess_type and pdata_type can be used as follows:    TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC) the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.\n\nReferences\n\nThis method unifies the implementation of trust-region and adaptive regularization with cubics as described in\n\nDussault, J.-P. (2020).\nA unified efficient implementation of trust-region type algorithms for unconstrained optimization.\nINFOR: Information Systems and Operational Research, 58(2), 290-309.\n10.1080/03155986.2019.1624490\n\nExamples\n\nusing AdaptiveRegularization, ADNLPModels\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nstats = TRARC(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = solve!(solver, nlp)\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = GenericExecutionStats(nlp)\nstats = solve!(solver, nlp, stats)\n\n\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.ARCqKOpGN-Tuple{Stopping.NLPStopping}","page":"Reference","title":"AdaptiveRegularization.ARCqKOpGN","text":"TRARC(nlp; kwargs...)\n\nCompute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.\n\nSome variants of TRARC are already implemented and listed in AdaptiveRegularization.ALL_solvers.\n\nArguments\n\nnlp::AbstractNLPModel: the model solved, see NLPModels.jl.\n\nThe keyword arguments include\n\nTR::ARTrustRegion: structure with trust-region/ARC parameters, see SolverTools.jl. Default: ARTrustRegion(T(10.0)).\nhess_type::Type{Hess}: Structure used to handle the hessian. The possible values are: HessDense, HessSparse, HessSparseCOO, HessOp. Default: HessOp.\npdata_type::Type{ParamData} Structure used for the preprocessing step. Default: PDataKARC.\nrobust::Bool: true implements a robust evaluation of the model. Default: true.\nverbose::Bool: true prints iteration information. Default: false.\n\nAdditional kwargs are used for stopping criterion, see Stopping.jl.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.stp: stopping object used for the algorithm;\nsolver.workspace: additional allocations;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nThis implementation uses Stopping.jl. Therefore, it is also possible to used\n\nTRARC(stp; kwargs...)\n\nwhich returns the stp::NLPStopping updated.\n\nFor advanced usage, first define a TRARCSolver to preallocate the  memory used in the algorithm, and then call solve!:\n\nstats = solve!(solver, nlp)\nstats = solve!(solver, nlp, stats)\n\nTo choose a particular variant, the keyword arguments hess_type and pdata_type can be used as follows:    TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC) the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.\n\nReferences\n\nThis method unifies the implementation of trust-region and adaptive regularization with cubics as described in\n\nDussault, J.-P. (2020).\nA unified efficient implementation of trust-region type algorithms for unconstrained optimization.\nINFOR: Information Systems and Operational Research, 58(2), 290-309.\n10.1080/03155986.2019.1624490\n\nExamples\n\nusing AdaptiveRegularization, ADNLPModels\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nstats = TRARC(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = solve!(solver, nlp)\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = GenericExecutionStats(nlp)\nstats = solve!(solver, nlp, stats)\n\n\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.ARCqKdense-Tuple{Stopping.NLPStopping}","page":"Reference","title":"AdaptiveRegularization.ARCqKdense","text":"TRARC(nlp; kwargs...)\n\nCompute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.\n\nSome variants of TRARC are already implemented and listed in AdaptiveRegularization.ALL_solvers.\n\nArguments\n\nnlp::AbstractNLPModel: the model solved, see NLPModels.jl.\n\nThe keyword arguments include\n\nTR::ARTrustRegion: structure with trust-region/ARC parameters, see SolverTools.jl. Default: ARTrustRegion(T(10.0)).\nhess_type::Type{Hess}: Structure used to handle the hessian. The possible values are: HessDense, HessSparse, HessSparseCOO, HessOp. Default: HessOp.\npdata_type::Type{ParamData} Structure used for the preprocessing step. Default: PDataKARC.\nrobust::Bool: true implements a robust evaluation of the model. Default: true.\nverbose::Bool: true prints iteration information. Default: false.\n\nAdditional kwargs are used for stopping criterion, see Stopping.jl.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.stp: stopping object used for the algorithm;\nsolver.workspace: additional allocations;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nThis implementation uses Stopping.jl. Therefore, it is also possible to used\n\nTRARC(stp; kwargs...)\n\nwhich returns the stp::NLPStopping updated.\n\nFor advanced usage, first define a TRARCSolver to preallocate the  memory used in the algorithm, and then call solve!:\n\nstats = solve!(solver, nlp)\nstats = solve!(solver, nlp, stats)\n\nTo choose a particular variant, the keyword arguments hess_type and pdata_type can be used as follows:    TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC) the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.\n\nReferences\n\nThis method unifies the implementation of trust-region and adaptive regularization with cubics as described in\n\nDussault, J.-P. (2020).\nA unified efficient implementation of trust-region type algorithms for unconstrained optimization.\nINFOR: Information Systems and Operational Research, 58(2), 290-309.\n10.1080/03155986.2019.1624490\n\nExamples\n\nusing AdaptiveRegularization, ADNLPModels\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nstats = TRARC(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = solve!(solver, nlp)\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = GenericExecutionStats(nlp)\nstats = solve!(solver, nlp, stats)\n\n\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.ARCqKsparse-Tuple{Stopping.NLPStopping}","page":"Reference","title":"AdaptiveRegularization.ARCqKsparse","text":"TRARC(nlp; kwargs...)\n\nCompute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.\n\nSome variants of TRARC are already implemented and listed in AdaptiveRegularization.ALL_solvers.\n\nArguments\n\nnlp::AbstractNLPModel: the model solved, see NLPModels.jl.\n\nThe keyword arguments include\n\nTR::ARTrustRegion: structure with trust-region/ARC parameters, see SolverTools.jl. Default: ARTrustRegion(T(10.0)).\nhess_type::Type{Hess}: Structure used to handle the hessian. The possible values are: HessDense, HessSparse, HessSparseCOO, HessOp. Default: HessOp.\npdata_type::Type{ParamData} Structure used for the preprocessing step. Default: PDataKARC.\nrobust::Bool: true implements a robust evaluation of the model. Default: true.\nverbose::Bool: true prints iteration information. Default: false.\n\nAdditional kwargs are used for stopping criterion, see Stopping.jl.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.stp: stopping object used for the algorithm;\nsolver.workspace: additional allocations;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nThis implementation uses Stopping.jl. Therefore, it is also possible to used\n\nTRARC(stp; kwargs...)\n\nwhich returns the stp::NLPStopping updated.\n\nFor advanced usage, first define a TRARCSolver to preallocate the  memory used in the algorithm, and then call solve!:\n\nstats = solve!(solver, nlp)\nstats = solve!(solver, nlp, stats)\n\nTo choose a particular variant, the keyword arguments hess_type and pdata_type can be used as follows:    TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC) the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.\n\nReferences\n\nThis method unifies the implementation of trust-region and adaptive regularization with cubics as described in\n\nDussault, J.-P. (2020).\nA unified efficient implementation of trust-region type algorithms for unconstrained optimization.\nINFOR: Information Systems and Operational Research, 58(2), 290-309.\n10.1080/03155986.2019.1624490\n\nExamples\n\nusing AdaptiveRegularization, ADNLPModels\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nstats = TRARC(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = solve!(solver, nlp)\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = GenericExecutionStats(nlp)\nstats = solve!(solver, nlp, stats)\n\n\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.ST_TROp-Tuple{Stopping.NLPStopping}","page":"Reference","title":"AdaptiveRegularization.ST_TROp","text":"TRARC(nlp; kwargs...)\n\nCompute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.\n\nSome variants of TRARC are already implemented and listed in AdaptiveRegularization.ALL_solvers.\n\nArguments\n\nnlp::AbstractNLPModel: the model solved, see NLPModels.jl.\n\nThe keyword arguments include\n\nTR::ARTrustRegion: structure with trust-region/ARC parameters, see SolverTools.jl. Default: ARTrustRegion(T(10.0)).\nhess_type::Type{Hess}: Structure used to handle the hessian. The possible values are: HessDense, HessSparse, HessSparseCOO, HessOp. Default: HessOp.\npdata_type::Type{ParamData} Structure used for the preprocessing step. Default: PDataKARC.\nrobust::Bool: true implements a robust evaluation of the model. Default: true.\nverbose::Bool: true prints iteration information. Default: false.\n\nAdditional kwargs are used for stopping criterion, see Stopping.jl.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.stp: stopping object used for the algorithm;\nsolver.workspace: additional allocations;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nThis implementation uses Stopping.jl. Therefore, it is also possible to used\n\nTRARC(stp; kwargs...)\n\nwhich returns the stp::NLPStopping updated.\n\nFor advanced usage, first define a TRARCSolver to preallocate the  memory used in the algorithm, and then call solve!:\n\nstats = solve!(solver, nlp)\nstats = solve!(solver, nlp, stats)\n\nTo choose a particular variant, the keyword arguments hess_type and pdata_type can be used as follows:    TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC) the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.\n\nReferences\n\nThis method unifies the implementation of trust-region and adaptive regularization with cubics as described in\n\nDussault, J.-P. (2020).\nA unified efficient implementation of trust-region type algorithms for unconstrained optimization.\nINFOR: Information Systems and Operational Research, 58(2), 290-309.\n10.1080/03155986.2019.1624490\n\nExamples\n\nusing AdaptiveRegularization, ADNLPModels\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nstats = TRARC(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = solve!(solver, nlp)\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = GenericExecutionStats(nlp)\nstats = solve!(solver, nlp, stats)\n\n\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.ST_TROpGN-Tuple{Stopping.NLPStopping}","page":"Reference","title":"AdaptiveRegularization.ST_TROpGN","text":"TRARC(nlp; kwargs...)\n\nCompute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.\n\nSome variants of TRARC are already implemented and listed in AdaptiveRegularization.ALL_solvers.\n\nArguments\n\nnlp::AbstractNLPModel: the model solved, see NLPModels.jl.\n\nThe keyword arguments include\n\nTR::ARTrustRegion: structure with trust-region/ARC parameters, see SolverTools.jl. Default: ARTrustRegion(T(10.0)).\nhess_type::Type{Hess}: Structure used to handle the hessian. The possible values are: HessDense, HessSparse, HessSparseCOO, HessOp. Default: HessOp.\npdata_type::Type{ParamData} Structure used for the preprocessing step. Default: PDataKARC.\nrobust::Bool: true implements a robust evaluation of the model. Default: true.\nverbose::Bool: true prints iteration information. Default: false.\n\nAdditional kwargs are used for stopping criterion, see Stopping.jl.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.stp: stopping object used for the algorithm;\nsolver.workspace: additional allocations;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nThis implementation uses Stopping.jl. Therefore, it is also possible to used\n\nTRARC(stp; kwargs...)\n\nwhich returns the stp::NLPStopping updated.\n\nFor advanced usage, first define a TRARCSolver to preallocate the  memory used in the algorithm, and then call solve!:\n\nstats = solve!(solver, nlp)\nstats = solve!(solver, nlp, stats)\n\nTo choose a particular variant, the keyword arguments hess_type and pdata_type can be used as follows:    TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC) the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.\n\nReferences\n\nThis method unifies the implementation of trust-region and adaptive regularization with cubics as described in\n\nDussault, J.-P. (2020).\nA unified efficient implementation of trust-region type algorithms for unconstrained optimization.\nINFOR: Information Systems and Operational Research, 58(2), 290-309.\n10.1080/03155986.2019.1624490\n\nExamples\n\nusing AdaptiveRegularization, ADNLPModels\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nstats = TRARC(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = solve!(solver, nlp)\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = GenericExecutionStats(nlp)\nstats = solve!(solver, nlp, stats)\n\n\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.ST_TROpGNLSCgls-Tuple{Stopping.NLPStopping}","page":"Reference","title":"AdaptiveRegularization.ST_TROpGNLSCgls","text":"TRARC(nlp; kwargs...)\n\nCompute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.\n\nSome variants of TRARC are already implemented and listed in AdaptiveRegularization.ALL_solvers.\n\nArguments\n\nnlp::AbstractNLPModel: the model solved, see NLPModels.jl.\n\nThe keyword arguments include\n\nTR::ARTrustRegion: structure with trust-region/ARC parameters, see SolverTools.jl. Default: ARTrustRegion(T(10.0)).\nhess_type::Type{Hess}: Structure used to handle the hessian. The possible values are: HessDense, HessSparse, HessSparseCOO, HessOp. Default: HessOp.\npdata_type::Type{ParamData} Structure used for the preprocessing step. Default: PDataKARC.\nrobust::Bool: true implements a robust evaluation of the model. Default: true.\nverbose::Bool: true prints iteration information. Default: false.\n\nAdditional kwargs are used for stopping criterion, see Stopping.jl.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.stp: stopping object used for the algorithm;\nsolver.workspace: additional allocations;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nThis implementation uses Stopping.jl. Therefore, it is also possible to used\n\nTRARC(stp; kwargs...)\n\nwhich returns the stp::NLPStopping updated.\n\nFor advanced usage, first define a TRARCSolver to preallocate the  memory used in the algorithm, and then call solve!:\n\nstats = solve!(solver, nlp)\nstats = solve!(solver, nlp, stats)\n\nTo choose a particular variant, the keyword arguments hess_type and pdata_type can be used as follows:    TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC) the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.\n\nReferences\n\nThis method unifies the implementation of trust-region and adaptive regularization with cubics as described in\n\nDussault, J.-P. (2020).\nA unified efficient implementation of trust-region type algorithms for unconstrained optimization.\nINFOR: Information Systems and Operational Research, 58(2), 290-309.\n10.1080/03155986.2019.1624490\n\nExamples\n\nusing AdaptiveRegularization, ADNLPModels\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nstats = TRARC(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = solve!(solver, nlp)\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = GenericExecutionStats(nlp)\nstats = solve!(solver, nlp, stats)\n\n\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.ST_TROpGNLSLsqr-Tuple{Stopping.NLPStopping}","page":"Reference","title":"AdaptiveRegularization.ST_TROpGNLSLsqr","text":"TRARC(nlp; kwargs...)\n\nCompute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.\n\nSome variants of TRARC are already implemented and listed in AdaptiveRegularization.ALL_solvers.\n\nArguments\n\nnlp::AbstractNLPModel: the model solved, see NLPModels.jl.\n\nThe keyword arguments include\n\nTR::ARTrustRegion: structure with trust-region/ARC parameters, see SolverTools.jl. Default: ARTrustRegion(T(10.0)).\nhess_type::Type{Hess}: Structure used to handle the hessian. The possible values are: HessDense, HessSparse, HessSparseCOO, HessOp. Default: HessOp.\npdata_type::Type{ParamData} Structure used for the preprocessing step. Default: PDataKARC.\nrobust::Bool: true implements a robust evaluation of the model. Default: true.\nverbose::Bool: true prints iteration information. Default: false.\n\nAdditional kwargs are used for stopping criterion, see Stopping.jl.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.stp: stopping object used for the algorithm;\nsolver.workspace: additional allocations;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nThis implementation uses Stopping.jl. Therefore, it is also possible to used\n\nTRARC(stp; kwargs...)\n\nwhich returns the stp::NLPStopping updated.\n\nFor advanced usage, first define a TRARCSolver to preallocate the  memory used in the algorithm, and then call solve!:\n\nstats = solve!(solver, nlp)\nstats = solve!(solver, nlp, stats)\n\nTo choose a particular variant, the keyword arguments hess_type and pdata_type can be used as follows:    TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC) the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.\n\nReferences\n\nThis method unifies the implementation of trust-region and adaptive regularization with cubics as described in\n\nDussault, J.-P. (2020).\nA unified efficient implementation of trust-region type algorithms for unconstrained optimization.\nINFOR: Information Systems and Operational Research, 58(2), 290-309.\n10.1080/03155986.2019.1624490\n\nExamples\n\nusing AdaptiveRegularization, ADNLPModels\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nstats = TRARC(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = solve!(solver, nlp)\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = GenericExecutionStats(nlp)\nstats = solve!(solver, nlp, stats)\n\n\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.ST_TROpLS-Tuple{Stopping.NLPStopping}","page":"Reference","title":"AdaptiveRegularization.ST_TROpLS","text":"TRARC(nlp; kwargs...)\n\nCompute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.\n\nSome variants of TRARC are already implemented and listed in AdaptiveRegularization.ALL_solvers.\n\nArguments\n\nnlp::AbstractNLPModel: the model solved, see NLPModels.jl.\n\nThe keyword arguments include\n\nTR::ARTrustRegion: structure with trust-region/ARC parameters, see SolverTools.jl. Default: ARTrustRegion(T(10.0)).\nhess_type::Type{Hess}: Structure used to handle the hessian. The possible values are: HessDense, HessSparse, HessSparseCOO, HessOp. Default: HessOp.\npdata_type::Type{ParamData} Structure used for the preprocessing step. Default: PDataKARC.\nrobust::Bool: true implements a robust evaluation of the model. Default: true.\nverbose::Bool: true prints iteration information. Default: false.\n\nAdditional kwargs are used for stopping criterion, see Stopping.jl.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.stp: stopping object used for the algorithm;\nsolver.workspace: additional allocations;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nThis implementation uses Stopping.jl. Therefore, it is also possible to used\n\nTRARC(stp; kwargs...)\n\nwhich returns the stp::NLPStopping updated.\n\nFor advanced usage, first define a TRARCSolver to preallocate the  memory used in the algorithm, and then call solve!:\n\nstats = solve!(solver, nlp)\nstats = solve!(solver, nlp, stats)\n\nTo choose a particular variant, the keyword arguments hess_type and pdata_type can be used as follows:    TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC) the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.\n\nReferences\n\nThis method unifies the implementation of trust-region and adaptive regularization with cubics as described in\n\nDussault, J.-P. (2020).\nA unified efficient implementation of trust-region type algorithms for unconstrained optimization.\nINFOR: Information Systems and Operational Research, 58(2), 290-309.\n10.1080/03155986.2019.1624490\n\nExamples\n\nusing AdaptiveRegularization, ADNLPModels\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nstats = TRARC(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = solve!(solver, nlp)\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = GenericExecutionStats(nlp)\nstats = solve!(solver, nlp, stats)\n\n\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.ST_TRdense-Tuple{Stopping.NLPStopping}","page":"Reference","title":"AdaptiveRegularization.ST_TRdense","text":"TRARC(nlp; kwargs...)\n\nCompute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.\n\nSome variants of TRARC are already implemented and listed in AdaptiveRegularization.ALL_solvers.\n\nArguments\n\nnlp::AbstractNLPModel: the model solved, see NLPModels.jl.\n\nThe keyword arguments include\n\nTR::ARTrustRegion: structure with trust-region/ARC parameters, see SolverTools.jl. Default: ARTrustRegion(T(10.0)).\nhess_type::Type{Hess}: Structure used to handle the hessian. The possible values are: HessDense, HessSparse, HessSparseCOO, HessOp. Default: HessOp.\npdata_type::Type{ParamData} Structure used for the preprocessing step. Default: PDataKARC.\nrobust::Bool: true implements a robust evaluation of the model. Default: true.\nverbose::Bool: true prints iteration information. Default: false.\n\nAdditional kwargs are used for stopping criterion, see Stopping.jl.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.stp: stopping object used for the algorithm;\nsolver.workspace: additional allocations;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nThis implementation uses Stopping.jl. Therefore, it is also possible to used\n\nTRARC(stp; kwargs...)\n\nwhich returns the stp::NLPStopping updated.\n\nFor advanced usage, first define a TRARCSolver to preallocate the  memory used in the algorithm, and then call solve!:\n\nstats = solve!(solver, nlp)\nstats = solve!(solver, nlp, stats)\n\nTo choose a particular variant, the keyword arguments hess_type and pdata_type can be used as follows:    TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC) the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.\n\nReferences\n\nThis method unifies the implementation of trust-region and adaptive regularization with cubics as described in\n\nDussault, J.-P. (2020).\nA unified efficient implementation of trust-region type algorithms for unconstrained optimization.\nINFOR: Information Systems and Operational Research, 58(2), 290-309.\n10.1080/03155986.2019.1624490\n\nExamples\n\nusing AdaptiveRegularization, ADNLPModels\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nstats = TRARC(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = solve!(solver, nlp)\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = GenericExecutionStats(nlp)\nstats = solve!(solver, nlp, stats)\n\n\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.ST_TRsparse-Tuple{Stopping.NLPStopping}","page":"Reference","title":"AdaptiveRegularization.ST_TRsparse","text":"TRARC(nlp; kwargs...)\n\nCompute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.\n\nSome variants of TRARC are already implemented and listed in AdaptiveRegularization.ALL_solvers.\n\nArguments\n\nnlp::AbstractNLPModel: the model solved, see NLPModels.jl.\n\nThe keyword arguments include\n\nTR::ARTrustRegion: structure with trust-region/ARC parameters, see SolverTools.jl. Default: ARTrustRegion(T(10.0)).\nhess_type::Type{Hess}: Structure used to handle the hessian. The possible values are: HessDense, HessSparse, HessSparseCOO, HessOp. Default: HessOp.\npdata_type::Type{ParamData} Structure used for the preprocessing step. Default: PDataKARC.\nrobust::Bool: true implements a robust evaluation of the model. Default: true.\nverbose::Bool: true prints iteration information. Default: false.\n\nAdditional kwargs are used for stopping criterion, see Stopping.jl.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.stp: stopping object used for the algorithm;\nsolver.workspace: additional allocations;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nThis implementation uses Stopping.jl. Therefore, it is also possible to used\n\nTRARC(stp; kwargs...)\n\nwhich returns the stp::NLPStopping updated.\n\nFor advanced usage, first define a TRARCSolver to preallocate the  memory used in the algorithm, and then call solve!:\n\nstats = solve!(solver, nlp)\nstats = solve!(solver, nlp, stats)\n\nTo choose a particular variant, the keyword arguments hess_type and pdata_type can be used as follows:    TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC) the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.\n\nReferences\n\nThis method unifies the implementation of trust-region and adaptive regularization with cubics as described in\n\nDussault, J.-P. (2020).\nA unified efficient implementation of trust-region type algorithms for unconstrained optimization.\nINFOR: Information Systems and Operational Research, 58(2), 290-309.\n10.1080/03155986.2019.1624490\n\nExamples\n\nusing AdaptiveRegularization, ADNLPModels\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nstats = TRARC(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = solve!(solver, nlp)\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = GenericExecutionStats(nlp)\nstats = solve!(solver, nlp, stats)\n\n\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.TRARC","page":"Reference","title":"AdaptiveRegularization.TRARC","text":"TRARC(nlp; kwargs...)\n\nCompute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.\n\nSome variants of TRARC are already implemented and listed in AdaptiveRegularization.ALL_solvers.\n\nArguments\n\nnlp::AbstractNLPModel: the model solved, see NLPModels.jl.\n\nThe keyword arguments include\n\nTR::ARTrustRegion: structure with trust-region/ARC parameters, see SolverTools.jl. Default: ARTrustRegion(T(10.0)).\nhess_type::Type{Hess}: Structure used to handle the hessian. The possible values are: HessDense, HessSparse, HessSparseCOO, HessOp. Default: HessOp.\npdata_type::Type{ParamData} Structure used for the preprocessing step. Default: PDataKARC.\nrobust::Bool: true implements a robust evaluation of the model. Default: true.\nverbose::Bool: true prints iteration information. Default: false.\n\nAdditional kwargs are used for stopping criterion, see Stopping.jl.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.stp: stopping object used for the algorithm;\nsolver.workspace: additional allocations;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nThis implementation uses Stopping.jl. Therefore, it is also possible to used\n\nTRARC(stp; kwargs...)\n\nwhich returns the stp::NLPStopping updated.\n\nFor advanced usage, first define a TRARCSolver to preallocate the  memory used in the algorithm, and then call solve!:\n\nstats = solve!(solver, nlp)\nstats = solve!(solver, nlp, stats)\n\nTo choose a particular variant, the keyword arguments hess_type and pdata_type can be used as follows:    TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC) the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.\n\nReferences\n\nThis method unifies the implementation of trust-region and adaptive regularization with cubics as described in\n\nDussault, J.-P. (2020).\nA unified efficient implementation of trust-region type algorithms for unconstrained optimization.\nINFOR: Information Systems and Operational Research, 58(2), 290-309.\n10.1080/03155986.2019.1624490\n\nExamples\n\nusing AdaptiveRegularization, ADNLPModels\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nstats = TRARC(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = solve!(solver, nlp)\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = GenericExecutionStats(nlp)\nstats = solve!(solver, nlp, stats)\n\n\n\n\n\n","category":"function"},{"location":"95-reference/#AdaptiveRegularization.TRKOp-Tuple{Stopping.NLPStopping}","page":"Reference","title":"AdaptiveRegularization.TRKOp","text":"TRARC(nlp; kwargs...)\n\nCompute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.\n\nSome variants of TRARC are already implemented and listed in AdaptiveRegularization.ALL_solvers.\n\nArguments\n\nnlp::AbstractNLPModel: the model solved, see NLPModels.jl.\n\nThe keyword arguments include\n\nTR::ARTrustRegion: structure with trust-region/ARC parameters, see SolverTools.jl. Default: ARTrustRegion(T(10.0)).\nhess_type::Type{Hess}: Structure used to handle the hessian. The possible values are: HessDense, HessSparse, HessSparseCOO, HessOp. Default: HessOp.\npdata_type::Type{ParamData} Structure used for the preprocessing step. Default: PDataKARC.\nrobust::Bool: true implements a robust evaluation of the model. Default: true.\nverbose::Bool: true prints iteration information. Default: false.\n\nAdditional kwargs are used for stopping criterion, see Stopping.jl.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.stp: stopping object used for the algorithm;\nsolver.workspace: additional allocations;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nThis implementation uses Stopping.jl. Therefore, it is also possible to used\n\nTRARC(stp; kwargs...)\n\nwhich returns the stp::NLPStopping updated.\n\nFor advanced usage, first define a TRARCSolver to preallocate the  memory used in the algorithm, and then call solve!:\n\nstats = solve!(solver, nlp)\nstats = solve!(solver, nlp, stats)\n\nTo choose a particular variant, the keyword arguments hess_type and pdata_type can be used as follows:    TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC) the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.\n\nReferences\n\nThis method unifies the implementation of trust-region and adaptive regularization with cubics as described in\n\nDussault, J.-P. (2020).\nA unified efficient implementation of trust-region type algorithms for unconstrained optimization.\nINFOR: Information Systems and Operational Research, 58(2), 290-309.\n10.1080/03155986.2019.1624490\n\nExamples\n\nusing AdaptiveRegularization, ADNLPModels\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nstats = TRARC(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = solve!(solver, nlp)\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = GenericExecutionStats(nlp)\nstats = solve!(solver, nlp, stats)\n\n\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.TRKdense-Tuple{Stopping.NLPStopping}","page":"Reference","title":"AdaptiveRegularization.TRKdense","text":"TRARC(nlp; kwargs...)\n\nCompute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.\n\nSome variants of TRARC are already implemented and listed in AdaptiveRegularization.ALL_solvers.\n\nArguments\n\nnlp::AbstractNLPModel: the model solved, see NLPModels.jl.\n\nThe keyword arguments include\n\nTR::ARTrustRegion: structure with trust-region/ARC parameters, see SolverTools.jl. Default: ARTrustRegion(T(10.0)).\nhess_type::Type{Hess}: Structure used to handle the hessian. The possible values are: HessDense, HessSparse, HessSparseCOO, HessOp. Default: HessOp.\npdata_type::Type{ParamData} Structure used for the preprocessing step. Default: PDataKARC.\nrobust::Bool: true implements a robust evaluation of the model. Default: true.\nverbose::Bool: true prints iteration information. Default: false.\n\nAdditional kwargs are used for stopping criterion, see Stopping.jl.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.stp: stopping object used for the algorithm;\nsolver.workspace: additional allocations;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nThis implementation uses Stopping.jl. Therefore, it is also possible to used\n\nTRARC(stp; kwargs...)\n\nwhich returns the stp::NLPStopping updated.\n\nFor advanced usage, first define a TRARCSolver to preallocate the  memory used in the algorithm, and then call solve!:\n\nstats = solve!(solver, nlp)\nstats = solve!(solver, nlp, stats)\n\nTo choose a particular variant, the keyword arguments hess_type and pdata_type can be used as follows:    TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC) the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.\n\nReferences\n\nThis method unifies the implementation of trust-region and adaptive regularization with cubics as described in\n\nDussault, J.-P. (2020).\nA unified efficient implementation of trust-region type algorithms for unconstrained optimization.\nINFOR: Information Systems and Operational Research, 58(2), 290-309.\n10.1080/03155986.2019.1624490\n\nExamples\n\nusing AdaptiveRegularization, ADNLPModels\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nstats = TRARC(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = solve!(solver, nlp)\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = GenericExecutionStats(nlp)\nstats = solve!(solver, nlp, stats)\n\n\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.TRKsparse-Tuple{Stopping.NLPStopping}","page":"Reference","title":"AdaptiveRegularization.TRKsparse","text":"TRARC(nlp; kwargs...)\n\nCompute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.\n\nSome variants of TRARC are already implemented and listed in AdaptiveRegularization.ALL_solvers.\n\nArguments\n\nnlp::AbstractNLPModel: the model solved, see NLPModels.jl.\n\nThe keyword arguments include\n\nTR::ARTrustRegion: structure with trust-region/ARC parameters, see SolverTools.jl. Default: ARTrustRegion(T(10.0)).\nhess_type::Type{Hess}: Structure used to handle the hessian. The possible values are: HessDense, HessSparse, HessSparseCOO, HessOp. Default: HessOp.\npdata_type::Type{ParamData} Structure used for the preprocessing step. Default: PDataKARC.\nrobust::Bool: true implements a robust evaluation of the model. Default: true.\nverbose::Bool: true prints iteration information. Default: false.\n\nAdditional kwargs are used for stopping criterion, see Stopping.jl.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.stp: stopping object used for the algorithm;\nsolver.workspace: additional allocations;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nThis implementation uses Stopping.jl. Therefore, it is also possible to used\n\nTRARC(stp; kwargs...)\n\nwhich returns the stp::NLPStopping updated.\n\nFor advanced usage, first define a TRARCSolver to preallocate the  memory used in the algorithm, and then call solve!:\n\nstats = solve!(solver, nlp)\nstats = solve!(solver, nlp, stats)\n\nTo choose a particular variant, the keyword arguments hess_type and pdata_type can be used as follows:    TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC) the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.\n\nReferences\n\nThis method unifies the implementation of trust-region and adaptive regularization with cubics as described in\n\nDussault, J.-P. (2020).\nA unified efficient implementation of trust-region type algorithms for unconstrained optimization.\nINFOR: Information Systems and Operational Research, 58(2), 290-309.\n10.1080/03155986.2019.1624490\n\nExamples\n\nusing AdaptiveRegularization, ADNLPModels\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nstats = TRARC(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = solve!(solver, nlp)\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = GenericExecutionStats(nlp)\nstats = solve!(solver, nlp, stats)\n\n\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.compute_q-NTuple{4, Any}","page":"Reference","title":"AdaptiveRegularization.compute_q","text":"compute_q(Hx, d, f)\n\nUpdate q = -(f + 0.5 * (Hx * d))  d in-place.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.decrease-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData, T, SolverTools.ARTrustRegion}} where T","page":"Reference","title":"AdaptiveRegularization.decrease","text":"decrease(X::TPData, ::T, TR::ARTrustRegion)\n\nReturn a decreased .\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.hessian!-Tuple{AdaptiveRegularization.AbstractHess, Any, Any}","page":"Reference","title":"AdaptiveRegularization.hessian!","text":"hessian!(workspace, nlp, x)\n\nReturn the Hessian matrix of nlp at x in-place with memory update of workspace.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.increase-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData, T, SolverTools.ARTrustRegion}} where T","page":"Reference","title":"AdaptiveRegularization.increase","text":"increase(X::TPData, ::T, TR::ARTrustRegion)\n\nReturn an increased .\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.init-Union{Tuple{S}, Tuple{T}, Tuple{Hess}, Tuple{Type{Hess}, NLPModels.AbstractNLPModel{T, S}, Any}} where {Hess<:AdaptiveRegularization.AbstractHess, T, S}","page":"Reference","title":"AdaptiveRegularization.init","text":"init(::Type{Hess}, nlp::AbstractNLPModel{T,S}, n)\n\nReturn the hessian structure Hess and its composite type.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.preprocess!-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData{T}, Vararg{Any, 6}}} where T","page":"Reference","title":"AdaptiveRegularization.preprocess!","text":"preprocess!(PData::TPData, H, g, gNorm2, n1, n2, )\n\nFunction called in the TRARC algorithm every time a new iterate has been accepted.\n\nArguments\n\nPData::TPData: data structure used for preprocessing.\nH: current Hessian matrix.\ng: current gradient.\ngNorm2: 2-norm of the gradient.\nn1: Current count on the number of Hessian-vector products.\nn2: Maximum number of Hessian-vector products accepted.\n: current value of the TR/ARC parameter.\n\nIt returns PData.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#AdaptiveRegularization.solve_model!-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData{T}, Vararg{Any, 6}}} where T","page":"Reference","title":"AdaptiveRegularization.solve_model!","text":"solve_model!(PData::TPData, H, g, gNorm2, n1, n2, )\n\nFunction called in the TRARC algorithm to solve the subproblem.\n\nArguments\n\nPData::TPData: data structure used for preprocessing.\nH: current Hessian matrix.\ng: current gradient.\ngNorm2: 2-norm of the gradient.\nn1: Current count on the number of Hessian-vector products.\nn2: Maximum number of Hessian-vector products accepted.\n: current value of the TR/ARC parameter.\n\nIt returns a couple (PData.d, PData.).\n\n\n\n\n\n","category":"method"},{"location":"2-benchmark/#Benchmarks","page":"Benchmarks","title":"Benchmarks","text":"","category":"section"},{"location":"2-benchmark/#CUTEst-benchmark","page":"Benchmarks","title":"CUTEst benchmark","text":"","category":"section"},{"location":"2-benchmark/","page":"Benchmarks","title":"Benchmarks","text":"With a JSO-compliant solver, such as DCI, we can run the solver on a set of problems, explore the results, and compare to other JSO-compliant solvers using specialized benchmark tools. We are following here the tutorial in SolverBenchmark.jl to run benchmarks on JSO-compliant solvers.","category":"page"},{"location":"2-benchmark/","page":"Benchmarks","title":"Benchmarks","text":"using CUTEst","category":"page"},{"location":"2-benchmark/","page":"Benchmarks","title":"Benchmarks","text":"To test the implementation of DCI, we use the package CUTEst.jl, which implements CUTEstModel an instance of AbstractNLPModel.","category":"page"},{"location":"2-benchmark/","page":"Benchmarks","title":"Benchmarks","text":"using SolverBenchmark","category":"page"},{"location":"2-benchmark/","page":"Benchmarks","title":"Benchmarks","text":"Let us select unconstrained problems from CUTEst with a maximum of 300 variables.","category":"page"},{"location":"2-benchmark/","page":"Benchmarks","title":"Benchmarks","text":"nmax = 100\npnames = CUTEst.select(contype = \"unc\", max_var = nmax)\n\ncutest_problems = (CUTEstModel(p) for p in pnames)\n\nlength(cutest_problems) # number of problems","category":"page"},{"location":"2-benchmark/","page":"Benchmarks","title":"Benchmarks","text":"We compare here AdaptiveRegularization with trunk from JSOSolvers.jl on a subset of CUTEst problems.","category":"page"},{"location":"2-benchmark/","page":"Benchmarks","title":"Benchmarks","text":"using AdaptiveRegularization, JSOSolvers\n\n#Same time limit for all the solvers\nmax_time = 60. #20 minutes\natol, rtol = 1e-5, 1e-6\n\nsolvers = Dict(\n  :trunk => nlp -> trunk(\n    nlp,\n    max_time = max_time,\n    max_iter = typemax(Int64),\n    max_eval = typemax(Int64),\n    atol = atol,\n    rtol = rtol,\n  ),\n  :ARCqK => nlp -> ARCqKOp(\n    nlp,\n    max_time = max_time,\n    max_iter = typemax(Int64),\n    max_eval = typemax(Int64),\n    atol = atol,\n    rtol = rtol,\n  ),\n)\n\nstats = bmark_solvers(solvers, cutest_problems)","category":"page"},{"location":"2-benchmark/","page":"Benchmarks","title":"Benchmarks","text":"The function bmark_solvers return a Dict of DataFrames with detailed information on the execution. This output can be saved in a data file.","category":"page"},{"location":"2-benchmark/","page":"Benchmarks","title":"Benchmarks","text":"using JLD2\n@save \"trunk_arcqk_$(string(length(pnames))).jld2\" stats","category":"page"},{"location":"2-benchmark/","page":"Benchmarks","title":"Benchmarks","text":"The result of the benchmark can be explored via tables,","category":"page"},{"location":"2-benchmark/","page":"Benchmarks","title":"Benchmarks","text":"pretty_stats(stats[:ARCqK])","category":"page"},{"location":"2-benchmark/","page":"Benchmarks","title":"Benchmarks","text":"or it can also be used to make performance profiles.","category":"page"},{"location":"2-benchmark/","page":"Benchmarks","title":"Benchmarks","text":"using Plots\ngr()\n\nlegend = Dict(\n  :neval_obj => \"number of f evals\",\n  :neval_cons => \"number of c evals\",\n  :neval_grad => \"number of f evals\",\n  :neval_jac => \"number of c evals\",\n  :neval_jprod => \"number of c*v evals\",\n  :neval_jtprod  => \"number of c*v evals\",\n  :neval_hess  => \"number of f evals\",\n  :neval_hprod => \"number of f*v evals\",\n  :elapsed_time => \"elapsed time\"\n)\nperf_title(col) = \"Performance profile on CUTEst w.r.t. $(string(legend[col]))\"\n\nstyles = [:solid,:dash,:dot,:dashdot] #[:auto, :solid, :dash, :dot, :dashdot, :dashdotdot]\n\nfunction print_pp_column(col::Symbol, stats)\n\n   = minimum(minimum(filter(x -> x > 0, df[!, col])) for df in values(stats))\n  first_order(df) = df.status .== :first_order\n  unbounded(df) = df.status .== :unbounded\n  solved(df) = first_order(df) .| unbounded(df)\n  cost(df) = (max.(df[!, col], ) + .!solved(df) .* Inf)\n\n  p = performance_profile(\n    stats,\n    cost,\n    title=perf_title(col),\n    legend=:bottomright,\n    linestyles=styles\n  )\nend\n\nprint_pp_column(:elapsed_time, stats) # with respect to time","category":"page"},{"location":"2-benchmark/","page":"Benchmarks","title":"Benchmarks","text":"print_pp_column(:neval_hprod, stats) # with respect to number of Hession-vector products","category":"page"},{"location":"1-tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"1-tutorial/","page":"Tutorial","title":"Tutorial","text":"We show here the basic features of the package.","category":"page"},{"location":"1-tutorial/","page":"Tutorial","title":"Tutorial","text":"using AdaptiveRegularization, ADNLPModels\n\n# Rosenbrock\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0])\nstats = ARCqKOp(nlp, verbose = true)","category":"page"},{"location":"1-tutorial/","page":"Tutorial","title":"Tutorial","text":"It is possible to access the number of evaluations of each function of the NLPModel API using the following:","category":"page"},{"location":"1-tutorial/","page":"Tutorial","title":"Tutorial","text":"using NLPModels\n\nnobj = neval_obj(nlp) # return number of f call\nngra = neval_grad(nlp) # return number of gradient call\nnhes = neval_hess(nlp) # return number of Hessian call\nnhpr = neval_hprod(nlp) # return number of hessian-vector products\n\n(nobj, ngra, nhes, nhpr)","category":"page"},{"location":"1-tutorial/","page":"Tutorial","title":"Tutorial","text":"These functions come from the NLPModel API defined in NLPModels.jl. If you want to reset the internal counter, you just do reset!(nlp).","category":"page"},{"location":"90-contributing/#contributing","page":"Contributing guidelines","title":"Contributing guidelines","text":"","category":"section"},{"location":"90-contributing/","page":"Contributing guidelines","title":"Contributing guidelines","text":"First of all, thanks for the interest!","category":"page"},{"location":"90-contributing/","page":"Contributing guidelines","title":"Contributing guidelines","text":"We welcome all kinds of contribution, including, but not limited to code, documentation, examples, configuration, issue creating, etc.","category":"page"},{"location":"90-contributing/","page":"Contributing guidelines","title":"Contributing guidelines","text":"Be polite and respectful, and follow the code of conduct.","category":"page"},{"location":"90-contributing/#Bug-reports-and-discussions","page":"Contributing guidelines","title":"Bug reports and discussions","text":"","category":"section"},{"location":"90-contributing/","page":"Contributing guidelines","title":"Contributing guidelines","text":"If you think you found a bug, feel free to open an issue. Focused suggestions and requests can also be opened as issues. Before opening a pull request, start an issue or a discussion on the topic, please.","category":"page"},{"location":"90-contributing/#Working-on-an-issue","page":"Contributing guidelines","title":"Working on an issue","text":"","category":"section"},{"location":"90-contributing/","page":"Contributing guidelines","title":"Contributing guidelines","text":"If you found an issue that interests you, comment on that issue what your plans are. If the solution to the issue is clear, you can immediately create a pull request (see below). Otherwise, say what your proposed solution is and wait for a discussion around it.","category":"page"},{"location":"90-contributing/","page":"Contributing guidelines","title":"Contributing guidelines","text":"tip: Tip\nFeel free to ping us after a few days if there are no responses.","category":"page"},{"location":"90-contributing/","page":"Contributing guidelines","title":"Contributing guidelines","text":"If your solution involves code (or something that requires running the package locally), check the developer documentation. Otherwise, you can use the GitHub interface directly to create your pull request.","category":"page"},{"location":"3-doityourself/#Your-own-way","page":"Your own way","title":"Your own way","text":"","category":"section"},{"location":"3-doityourself/","page":"Your own way","title":"Your own way","text":"AdaptiveRegularization.jl implements an unified algorithm for trust-region methods and adaptive regularization with cubics. This package implements by default some variants, but anyone can design its own and benchmark it against existing ones.","category":"page"},{"location":"3-doityourself/","page":"Your own way","title":"Your own way","text":"using AdaptiveRegularization, Krylov","category":"page"},{"location":"3-doityourself/","page":"Your own way","title":"Your own way","text":"The implemented variants are accessible here:","category":"page"},{"location":"3-doityourself/","page":"Your own way","title":"Your own way","text":"AdaptiveRegularization.ALL_solvers","category":"page"},{"location":"3-doityourself/","page":"Your own way","title":"Your own way","text":"To make your own variant we need to implement:","category":"page"},{"location":"3-doityourself/","page":"Your own way","title":"Your own way","text":"A new data structure <: PData{T} for some real number type T.\nA preprocess!(PData::TPData, H, g, gNorm2, ) function called before each trust-region iteration.\nA solve_model!(PData::TPData, H, g, gNorm2, n1, n2, ::T) function used to solve the algorithm subproblem.","category":"page"},{"location":"3-doityourself/","page":"Your own way","title":"Your own way","text":"In the rest of this tutorial, we implement a Steihaug-Toint trust-region method using cg_lanczos from Krylov.jl to solve the linear subproblem with trust-region constraint.","category":"page"},{"location":"3-doityourself/","page":"Your own way","title":"Your own way","text":"mutable struct PDataST{S,T} <: AdaptiveRegularization.TPData{T}\n    d::S                      # Mandatory: solution of the subproblem\n    ::T                      # Mandatory\n    ::T                      # Inexact Newton order parameter: stop when ||q|| <  * ||g||^(1+)\n    ::T                      # Inexact Newton order parameter: stop when ||q|| <  * ||g||^(1+)\n    maxtol::T                 # Largest tolerance for Inexact Newton\n    mintol::T                 # Smallest tolerance for Inexact Newton\n    cgatol                    # Absolute tolerance for `cg_lanczos`\n    cgrtol                    # Relative tolerance for `cg_lanczos`\n\n    OK::Bool                  # Mandatory: preprocess success\n    solver::CgSolver          # Memory pre-allocation for `cg_lanczos`\nend","category":"page"},{"location":"3-doityourself/","page":"Your own way","title":"Your own way","text":"The TPData stuctures have a unified constructor with (::Type{S}, ::Type{T}, n) as arguments.","category":"page"},{"location":"3-doityourself/","page":"Your own way","title":"Your own way","text":"function PDataST(\n    ::Type{S},\n    ::Type{T},\n    n;\n     = T(0.5),\n     = T(0.01),\n    maxtol = T(0.01),\n    mintol = T(1.0e-8),\n    cgatol = (, , maxtol, mintol, gNorm2) -> max(mintol, min(maxtol,  * gNorm2^(1 + ))),\n    cgrtol = (, , maxtol, mintol, gNorm2) -> max(mintol, min(maxtol,  * gNorm2^)),\n    kwargs...,\n) where {S,T}\n    d = S(undef, n)\n     = zero(T)\n    OK = true\n    solver = CgSolver(n, n, S)\n    return PDataST(d, , , , maxtol, mintol, cgatol, cgrtol, OK, solver)\nend","category":"page"},{"location":"3-doityourself/","page":"Your own way","title":"Your own way","text":"For our Steihaug-Toint implementation, we do not run any preprocess operation, so we use the default one.","category":"page"},{"location":"3-doityourself/","page":"Your own way","title":"Your own way","text":"function AdaptiveRegularization.preprocess!(PData::AdaptiveRegularization.TPData, H, g, gNorm2, n1, n2, )\n    return PData\nend","category":"page"},{"location":"3-doityourself/","page":"Your own way","title":"Your own way","text":"We now solve the subproblem.","category":"page"},{"location":"3-doityourself/","page":"Your own way","title":"Your own way","text":"function AdaptiveRegularization.solve_model!(PData::PDataST, H, g, gNorm2, calls, max_calls, ::T) where {T}\n    , , maxtol, mintol = PData., PData., PData.maxtol, PData.mintol\n    n = length(g)\n    # precision = max(1e-12, min(0.5, (gNorm2^)))\n    # Tolerance used in Assumption 2.6b in the paper (  > 0, 0 <   1 )\n    cgatol = PData.cgatol(, , maxtol, mintol, gNorm2)\n    cgrtol = PData.cgrtol(, , maxtol, mintol, gNorm2)\n\n    solver = PData.solver\n    cg!(\n        solver,\n        H,\n        -g,\n        atol = cgatol,\n        rtol = cgrtol,\n        radius = ,\n        itmax = min(max_calls - sum(calls), max(2 * n, 50)),\n        verbose = 0,\n    )\n\n    PData.d .= solver.x\n    PData.OK = solver.stats.solved\n\n    return PData.d, PData.\nend","category":"page"},{"location":"3-doityourself/","page":"Your own way","title":"Your own way","text":"We can now proceed with the main solver call specifying the used pdata_type and solve_model. Since, Krylov.cg_lanczos only uses matrix-vector products, it is sufficient to evaluate the Hessian matrix as an operator, so we provide hess_type = HessOp.","category":"page"},{"location":"3-doityourself/","page":"Your own way","title":"Your own way","text":"ST_TROp(nlp; kwargs...) = TRARC(nlp, pdata_type = PDataST, hess_type = HessOp; kwargs...)","category":"page"},{"location":"3-doityourself/","page":"Your own way","title":"Your own way","text":"Finally, we can apply our new method to any NLPModels.","category":"page"},{"location":"3-doityourself/","page":"Your own way","title":"Your own way","text":"using ADNLPModels, OptimizationProblems\nnlp = OptimizationProblems.ADNLPProblems.arglina()\nST_TROp(nlp)","category":"page"},{"location":"3-doityourself/","page":"Your own way","title":"Your own way","text":"using ADNLPModels, NLPModels, OptimizationProblems, SolverBenchmark\n\nmeta = OptimizationProblems.meta\nproblems = meta[meta.variable_nvar .& (meta.ncon .== 0) .& .!(meta.has_bounds), :name]\nn = 150\nop_problems = (OptimizationProblems.ADNLPProblems.eval(Meta.parse(pb))(n = n) for pb in problems)\n\nmax_time = 120.0\nmax_ev = typemax(Int)\nmax_iter = typemax(Int)\natol = 1e-5\nrtol = 1e-6\n\nsolvers = Dict(\n    :ARCqKOp =>\n        nlp -> ARCqKOp(\n            nlp,\n            verbose = false,\n            atol = atol,\n            rtol = rtol,\n            max_time = max_time,\n            max_iter = max_iter,\n        ),\n    :ST_TROp =>\n        nlp -> ST_TROp(\n            nlp,\n            verbose = false,\n            atol = atol,\n            rtol = rtol,\n            max_time = max_time,\n            max_iter = max_iter,\n        ),\n)\nstats = bmark_solvers(solvers, op_problems)","category":"page"},{"location":"","page":"AdaptiveRegularization","title":"AdaptiveRegularization","text":"CurrentModule = AdaptiveRegularization","category":"page"},{"location":"#AdaptiveRegularization","page":"AdaptiveRegularization","title":"AdaptiveRegularization","text":"","category":"section"},{"location":"","page":"AdaptiveRegularization","title":"AdaptiveRegularization","text":"Documentation for AdaptiveRegularization.","category":"page"},{"location":"","page":"AdaptiveRegularization","title":"AdaptiveRegularization","text":"AdaptiveRegularization is a solver for unconstrained nonlinear problems,","category":"page"},{"location":"","page":"AdaptiveRegularization","title":"AdaptiveRegularization","text":"    min f(x)","category":"page"},{"location":"","page":"AdaptiveRegularization","title":"AdaptiveRegularization","text":"It uses other JuliaSmoothOptimizers packages for development. In particular, NLPModels.jl is used for defining the problem, and SolverCore.jl for the output.","category":"page"},{"location":"","page":"AdaptiveRegularization","title":"AdaptiveRegularization","text":"This package uses Stopping.jl via NLPStopping to handle its workflow, you can also see tutorials with Stopping to learn more.","category":"page"},{"location":"#Algorithm","page":"AdaptiveRegularization","title":"Algorithm","text":"","category":"section"},{"location":"","page":"AdaptiveRegularization","title":"AdaptiveRegularization","text":"The initial implementation of this package follows (Dussault, J.-P. 2020):","category":"page"},{"location":"","page":"AdaptiveRegularization","title":"AdaptiveRegularization","text":"*Adaptive cubic regularization (ARC) and trust-region (TR) methods use modified linear systems to compute their steps. The modified systems consist in adding some multiple of the identity matrix (or a well-chosen positive definite matrix) to the Hessian to obtain a sufficiently positive definite linear system, the so called shifted system. This type of system was first proposed by Levenberg and Marquardt. Some trial and error is often involved to obtain a specified value for this shift parameter. We provide an efficient unified implementation to track the shift parameter; our implementation encompasses many ARC and TR variants.","category":"page"},{"location":"#Contributors","page":"AdaptiveRegularization","title":"Contributors","text":"","category":"section"},{"location":"","page":"AdaptiveRegularization","title":"AdaptiveRegularization","text":"<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->","category":"page"}]
}
