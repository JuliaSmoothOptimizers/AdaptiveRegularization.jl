var documenterSearchIndex = {"docs":
[{"location":"doityourself/#Your-own-way","page":"Do it yourself","title":"Your own way","text":"","category":"section"},{"location":"doityourself/","page":"Do it yourself","title":"Do it yourself","text":"AdaptiveRegularization.jl implements an unified algorithm for trust-region methods and adaptive regularization with cubics. This package implements by default some variants, but anyone can design its own and benchmark it against existing ones.","category":"page"},{"location":"doityourself/","page":"Do it yourself","title":"Do it yourself","text":"using AdaptiveRegularization, Krylov","category":"page"},{"location":"doityourself/","page":"Do it yourself","title":"Do it yourself","text":"The implemented variants are accessible here:","category":"page"},{"location":"doityourself/","page":"Do it yourself","title":"Do it yourself","text":"AdaptiveRegularization.ALL_solvers","category":"page"},{"location":"doityourself/","page":"Do it yourself","title":"Do it yourself","text":"To make your own variant we need to implement:","category":"page"},{"location":"doityourself/","page":"Do it yourself","title":"Do it yourself","text":"A new data structure <: PData{T} for some real number type T.\nA preprocess!(PData::TPData, H, g, gNorm2, α) function called before each trust-region iteration.\nA solve_model!(PData::TPData, H, g, gNorm2, n1, n2, δ::T) function used to solve the algorithm subproblem.","category":"page"},{"location":"doityourself/","page":"Do it yourself","title":"Do it yourself","text":"In the rest of this tutorial, we implement a Steihaug-Toint trust-region method using cg_lanczos from Krylov.jl to solve the linear subproblem with trust-region constraint.","category":"page"},{"location":"doityourself/","page":"Do it yourself","title":"Do it yourself","text":"mutable struct PDataST{S,T} <: AdaptiveRegularization.TPData{T}\n    d::S                      # Mandatory: solution of the subproblem\n    λ::T                      # Mandatory\n    ζ::T                      # Inexact Newton order parameter: stop when ||∇q|| < ξ * ||g||^(1+ζ)\n    ξ::T                      # Inexact Newton order parameter: stop when ||∇q|| < ξ * ||g||^(1+ζ)\n    maxtol::T                 # Largest tolerance for Inexact Newton\n    mintol::T                 # Smallest tolerance for Inexact Newton\n    cgatol                    # Absolute tolerance for `cg_lanczos`\n    cgrtol                    # Relative tolerance for `cg_lanczos`\n\n    OK::Bool                  # Mandatory: preprocess success\n    solver::CgSolver          # Memory pre-allocation for `cg_lanczos`\nend","category":"page"},{"location":"doityourself/","page":"Do it yourself","title":"Do it yourself","text":"The TPData stuctures have a unified constructor with (::Type{S}, ::Type{T}, n) as arguments.","category":"page"},{"location":"doityourself/","page":"Do it yourself","title":"Do it yourself","text":"function PDataST(\n    ::Type{S},\n    ::Type{T},\n    n;\n    ζ = T(0.5),\n    ξ = T(0.01),\n    maxtol = T(0.01),\n    mintol = T(1.0e-8),\n    cgatol = (ζ, ξ, maxtol, mintol, gNorm2) -> max(mintol, min(maxtol, ξ * gNorm2^(1 + ζ))),\n    cgrtol = (ζ, ξ, maxtol, mintol, gNorm2) -> max(mintol, min(maxtol, ξ * gNorm2^ζ)),\n    kwargs...,\n) where {S,T}\n    d = S(undef, n)\n    λ = zero(T)\n    OK = true\n    solver = CgSolver(n, n, S)\n    return PDataST(d, λ, ζ, ξ, maxtol, mintol, cgatol, cgrtol, OK, solver)\nend","category":"page"},{"location":"doityourself/","page":"Do it yourself","title":"Do it yourself","text":"For our Steihaug-Toint implementation, we do not run any preprocess operation, so we use the default one.","category":"page"},{"location":"doityourself/","page":"Do it yourself","title":"Do it yourself","text":"function AdaptiveRegularization.preprocess!(PData::AdaptiveRegularization.TPData, H, g, gNorm2, n1, n2, α)\n    return PData\nend","category":"page"},{"location":"doityourself/","page":"Do it yourself","title":"Do it yourself","text":"We now solve the subproblem.","category":"page"},{"location":"doityourself/","page":"Do it yourself","title":"Do it yourself","text":"function AdaptiveRegularization.solve_model!(PData::PDataST, H, g, gNorm2, calls, max_calls, δ::T) where {T}\n    ζ, ξ, maxtol, mintol = PData.ζ, PData.ξ, PData.maxtol, PData.mintol\n    n = length(g)\n    # precision = max(1e-12, min(0.5, (gNorm2^ζ)))\n    # Tolerance used in Assumption 2.6b in the paper ( ξ > 0, 0 < ζ ≤ 1 )\n    cgatol = PData.cgatol(ζ, ξ, maxtol, mintol, gNorm2)\n    cgrtol = PData.cgrtol(ζ, ξ, maxtol, mintol, gNorm2)\n\n    solver = PData.solver\n    cg!(\n        solver,\n        H,\n        -g,\n        atol = cgatol,\n        rtol = cgrtol,\n        radius = δ,\n        itmax = min(max_calls - sum(calls), max(2 * n, 50)),\n        verbose = 0,\n    )\n\n    PData.d .= solver.x\n    PData.OK = solver.stats.solved\n\n    return PData.d, PData.λ\nend","category":"page"},{"location":"doityourself/","page":"Do it yourself","title":"Do it yourself","text":"We can now proceed with the main solver call specifying the used pdata_type and solve_model. Since, Krylov.cg_lanczos only uses matrix-vector products, it is sufficient to evaluate the Hessian matrix as an operator, so we provide hess_type = HessOp.","category":"page"},{"location":"doityourself/","page":"Do it yourself","title":"Do it yourself","text":"ST_TROp(nlp; kwargs...) = TRARC(nlp, pdata_type = PDataST, hess_type = HessOp; kwargs...)","category":"page"},{"location":"doityourself/","page":"Do it yourself","title":"Do it yourself","text":"Finally, we can apply our new method to any NLPModels.","category":"page"},{"location":"doityourself/","page":"Do it yourself","title":"Do it yourself","text":"using ADNLPModels, OptimizationProblems\nnlp = OptimizationProblems.ADNLPProblems.arglina()\nST_TROp(nlp)","category":"page"},{"location":"doityourself/","page":"Do it yourself","title":"Do it yourself","text":"using ADNLPModels, NLPModels, OptimizationProblems, SolverBenchmark\n\nmeta = OptimizationProblems.meta\nproblems = meta[meta.variable_nvar .& (meta.ncon .== 0) .& .!(meta.has_bounds), :name]\nn = 150\nop_problems = (OptimizationProblems.ADNLPProblems.eval(Meta.parse(pb))(n = n) for pb in problems)\n\nmax_time = 120.0\nmax_ev = typemax(Int)\nmax_iter = typemax(Int)\natol = 1e-5\nrtol = 1e-6\n\nsolvers = Dict(\n    :ARCqKOp =>\n        nlp -> ARCqKOp(\n            nlp,\n            verbose = false,\n            atol = atol,\n            rtol = rtol,\n            max_time = max_time,\n            max_iter = max_iter,\n        ),\n    :ST_TROp =>\n        nlp -> ST_TROp(\n            nlp,\n            verbose = false,\n            atol = atol,\n            rtol = rtol,\n            max_time = max_time,\n            max_iter = max_iter,\n        ),\n)\nstats = bmark_solvers(solvers, op_problems)","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [AdaptiveRegularization]","category":"page"},{"location":"reference/#AdaptiveRegularization.HessDense","page":"Reference","title":"AdaptiveRegularization.HessDense","text":"HessDense(::AbstractNLPModel{T,S}, n)\n\nReturn a structure used for the evaluation of dense Hessian matrix.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AdaptiveRegularization.HessGaussNewtonOp","page":"Reference","title":"AdaptiveRegularization.HessGaussNewtonOp","text":"HessGaussNewtonOp(::AbstractNLSModel{T,S}, n)\n\nReturn a structure used for the evaluation of the Hessian matrix as an operator.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AdaptiveRegularization.HessOp","page":"Reference","title":"AdaptiveRegularization.HessOp","text":"HessOp(::AbstractNLPModel{T,S}, n)\n\nReturn a structure used for the evaluation of the Hessian matrix as an operator.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AdaptiveRegularization.HessSparse","page":"Reference","title":"AdaptiveRegularization.HessSparse","text":"HessSparse(::AbstractNLPModel{T,S}, n)\n\nReturn a structure used for the evaluation of sparse Hessian matrix.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AdaptiveRegularization.HessSparseCOO","page":"Reference","title":"AdaptiveRegularization.HessSparseCOO","text":"HessSparseCOO(::AbstractNLPModel{T,S}, n)\n\nReturn a structure used for the evaluation of sparse Hessian matrix in COO-format.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AdaptiveRegularization.PDataKARC","page":"Reference","title":"AdaptiveRegularization.PDataKARC","text":"PDataKARC(::Type{S}, ::Type{T}, n)\n\nReturn a structure used for the preprocessing of ARCqK methods.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AdaptiveRegularization.PDataNLSST","page":"Reference","title":"AdaptiveRegularization.PDataNLSST","text":"PDataNLSST(::Type{S}, ::Type{T}, n)\n\nReturn a structure used for the preprocessing of Steihaug-Toint methods for Gauss-Newton approximation of nonlinear least squares.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AdaptiveRegularization.PDataST","page":"Reference","title":"AdaptiveRegularization.PDataST","text":"PDataST(::Type{S}, ::Type{T}, n)\n\nReturn a structure used for the preprocessing of Steihaug-Toint methods.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AdaptiveRegularization.PDataTRK","page":"Reference","title":"AdaptiveRegularization.PDataTRK","text":"PDataTRK(::Type{S}, ::Type{T}, n)\n\nReturn a structure used for the preprocessing of TRK methods.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AdaptiveRegularization.TRARCSolver","page":"Reference","title":"AdaptiveRegularization.TRARCSolver","text":"TRARCSolver(nlp::AbstractNLPModel [, x0 = nlp.meta.x0]; kwargs...)\nTRARCSolver(stp::NLPStopping; kwargs...)\n\nStructure regrouping all the structure used during the TRARC call. It returns a TRARCSolver structure.\n\nArguments\n\nThe keyword arguments may include:\n\nstp::NLPStopping: Stopping structure for this algorithm workflow;\nmeta::ParamData: see ParamData;\nworkspace::TRARCWorkspace: allocated space for the solver itself;\nTR::TrustRegion: trust-region parameters.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AdaptiveRegularization.TRARCWorkspace","page":"Reference","title":"AdaptiveRegularization.TRARCWorkspace","text":"TRARCWorkspace(nlp, ::Type{Hess}, n)\n\nPre-allocate the memory used during the TRARC call for the problem nlp of size n. The possible values for Hess are: HessDense, HessSparse, HessSparseCOO, HessOp. Return a TRARCWorkspace structure.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AdaptiveRegularization.TrustRegion","page":"Reference","title":"AdaptiveRegularization.TrustRegion","text":"TrustRegion(α₀::T;kwargs...)\n\nSelect the main parameters used in the TRARC algorithm with α₀ as initial TR/ARC parameter. The keyword arguments are:\n\nmax_α::T: Maximum value for α. Default T(1) / sqrt(eps(T)).\nacceptance_threshold::T: Ratio over which the step is successful. Default T(0.1).\nincrease_threshold::T: Ratio over which we increase α. Default T(0.75).\nreduce_threshold::T: Ratio under which we decrease α. Default T(0.1).\nincrease_factor::T: Factor of increase of α. Default T(5.0).\ndecrease_factor::T: Factor of decrease of α. Default T(0.1).\nmax_unsuccinarow::Int: Limit on the number of successive unsucessful iterations. Default 30.\n\nReturns a TrustRegion structure.\n\nThis can be compared to https://github.com/JuliaSmoothOptimizers/SolverTools.jl/blob/main/src/trust-region/basic-trust-region.jl\n\n\n\n\n\n","category":"type"},{"location":"reference/#AdaptiveRegularization.TrustRegionException","page":"Reference","title":"AdaptiveRegularization.TrustRegionException","text":"Exception type raised in case of error.\n\n\n\n\n\n","category":"type"},{"location":"reference/#AdaptiveRegularization.TRARC","page":"Reference","title":"AdaptiveRegularization.TRARC","text":"TRARC(nlp; kwargs...)\n\nCompute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.\n\nArguments\n\nnlp::AbstractNLPModel: the model solved, see NLPModels.jl.\n\nThe keyword arguments include\n\nTR::TrustRegion: structure with trust-region/ARC parameters, see TrustRegion. Default: TrustRegion(T(10.0)).\nhess_type::Type{Hess}: Structure used to handle the hessian. The possible values are: HessDense, HessSparse, HessSparseCOO, HessOp. Default: HessOp.\npdata_type::Type{ParamData} Structure used for the preprocessing step. Default: PDataKARC.\nrobust::Bool: true implements a robust evaluation of the model. Default: true.\nverbose::Bool: true prints iteration information. Default: false.\n\nAdditional kwargs are used for stopping criterion, see Stopping.jl.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nThis implementation uses Stopping.jl. Therefore, it is also possible to used\n\nTRARC(stp; kwargs...)\n\nwhich returns the stp::NLPStopping updated.\n\nFor advanced usage, the principal call to the solver uses a TRARCSolver.\n\nstats = solve!(solver, nlp)\nstats = solve!(solver, nlp, stats)\n\nSome variants of TRARC are already implemented and listed in AdaptiveRegularization.ALL_solvers.\n\nReferences\n\nThis method unifies the implementation of trust-region and adaptive regularization with cubics as described in\n\nDussault, J.-P. (2020).\nA unified efficient implementation of trust-region type algorithms for unconstrained optimization.\nINFOR: Information Systems and Operational Research, 58(2), 290-309.\n10.1080/03155986.2019.1624490\n\nExamples\n\nusing AdaptiveRegularization, ADNLPModels\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nstats = TRARC(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = solve!(solver, nlp)\n\nusing AdaptiveRegularization, ADNLPModels, SolverCore\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);\nsolver = TRARCSolver(nlp);\nstats = GenericExecutionStats(nlp)\nstats = solve!(solver, nlp, stats)\n\n\n\n\n\n","category":"function"},{"location":"reference/#AdaptiveRegularization.compute_r-Union{Tuple{T}, Tuple{Any, T, Vararg{Any, 7}}} where T","page":"Reference","title":"AdaptiveRegularization.compute_r","text":"compute_r(nlp, f, Δf, Δq, slope, d, xnext, gnext, robust)\n\nCompute the actual vs predicted reduction ratio ∆f/Δq.\n\nArguments:\n\nnlp: Current model we are trying to solve\nf: current objective value\nΔf: = f - f_trial is the actual reduction is an objective/merit/penalty function,\nΔq: q - q_trial is the reduction predicted by the model q of f.\nslope: current slope\nd: potential next direction\nxnext: potential next iterate\ngnext: current gradient value, if good_grad is true, then this value has been udpated.\nrobust: if true, try to trap potential cancellation errors\n\nOutput:\n\nr: reduction ratio ∆f/Δq\ngood_grad: true if gnext has been recomputed\ngnext: gradient.\n\nWe assume that qis being minimized, and therefore thatΔq > 0`.\n\n\n\n\n\n","category":"method"},{"location":"reference/#AdaptiveRegularization.compute_Δq-NTuple{4, Any}","page":"Reference","title":"AdaptiveRegularization.compute_Δq","text":"compute_Δq(Hx, d, ∇f)\n\nUpdate Δq = -(∇f + 0.5 * (Hx * d)) ⋅ d in-place.\n\n\n\n\n\n","category":"method"},{"location":"reference/#AdaptiveRegularization.decrease-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData, T, TrustRegion}} where T","page":"Reference","title":"AdaptiveRegularization.decrease","text":"decrease(X::TPData, α::T, TR::TrustRegion)\n\nReturn a decreased α.\n\n\n\n\n\n","category":"method"},{"location":"reference/#AdaptiveRegularization.hessian!","page":"Reference","title":"AdaptiveRegularization.hessian!","text":"hessian!(workspace::HessDense, nlp, x)\n\nReturn the Hessian matrix of nlp at x in-place with memory update of workspace.\n\n\n\n\n\n","category":"function"},{"location":"reference/#AdaptiveRegularization.increase-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData, T, TrustRegion}} where T","page":"Reference","title":"AdaptiveRegularization.increase","text":"increase(X::TPData, α::T, TR::TrustRegion)\n\nReturn an increased α.\n\n\n\n\n\n","category":"method"},{"location":"reference/#AdaptiveRegularization.init-Union{Tuple{S}, Tuple{T}, Tuple{Hess}, Tuple{Type{Hess}, NLPModels.AbstractNLPModel{T, S}, Any}} where {Hess, T, S}","page":"Reference","title":"AdaptiveRegularization.init","text":"init(::Type{Hess}, nlp::AbstractNLPModel{T,S}, n)\n\nReturn the hessian structure Hess and its composite type.\n\n\n\n\n\n","category":"method"},{"location":"reference/#AdaptiveRegularization.preprocess!-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData{T}, Vararg{Any, 6}}} where T","page":"Reference","title":"AdaptiveRegularization.preprocess!","text":"preprocess!(PData::TPData, H, g, gNorm2, n1, n2, α)\n\nFunction called in the TRARC algorithm every time a new iterate has been accepted.\n\nArguments\n\nPData::TPData: data structure used for preprocessing.\nH: current Hessian matrix.\ng: current gradient.\ngNorm2: 2-norm of the gradient.\nn1: Current count on the number of Hessian-vector products.\nn2: Maximum number of Hessian-vector products accepted.\nα: current value of the TR/ARC parameter.\n\nIt returns PData.\n\n\n\n\n\n","category":"method"},{"location":"reference/#AdaptiveRegularization.solve_model!-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData{T}, Vararg{Any, 6}}} where T","page":"Reference","title":"AdaptiveRegularization.solve_model!","text":"solve_model!(PData::TPData, H, g, gNorm2, n1, n2, α)\n\nFunction called in the TRARC algorithm to solve the subproblem.\n\nArguments\n\nPData::TPData: data structure used for preprocessing.\nH: current Hessian matrix.\ng: current gradient.\ngNorm2: 2-norm of the gradient.\nn1: Current count on the number of Hessian-vector products.\nn2: Maximum number of Hessian-vector products accepted.\nα: current value of the TR/ARC parameter.\n\nIt returns a couple (PData.d, PData.λ).\n\n\n\n\n\n","category":"method"},{"location":"benchmark/#Benchmarks","page":"Benchmark","title":"Benchmarks","text":"","category":"section"},{"location":"benchmark/#CUTEst-benchmark","page":"Benchmark","title":"CUTEst benchmark","text":"","category":"section"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"With a JSO-compliant solver, such as DCI, we can run the solver on a set of problems, explore the results, and compare to other JSO-compliant solvers using specialized benchmark tools.  We are following here the tutorial in SolverBenchmark.jl to run benchmarks on JSO-compliant solvers.","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"using CUTEst","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"To test the implementation of DCI, we use the package CUTEst.jl, which implements CUTEstModel an instance of AbstractNLPModel. ","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"using SolverBenchmark","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"Let us select unconstrained problems from CUTEst with a maximum of 300 variables.","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"nmax = 100\npnames = CUTEst.select(contype = \"unc\", max_var = nmax)\n\ncutest_problems = (CUTEstModel(p) for p in pnames)\n\nlength(cutest_problems) # number of problems","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"We compare here AdaptiveRegularization with trunk from JSOSolvers.jl on a subset of CUTEst problems.","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"using AdaptiveRegularization, JSOSolvers\n\n#Same time limit for all the solvers\nmax_time = 60. #20 minutes\natol, rtol = 1e-5, 1e-6\n\nsolvers = Dict(\n  :trunk => nlp -> trunk(\n    nlp,\n    max_time = max_time,\n    max_iter = typemax(Int64),\n    max_eval = typemax(Int64),\n    atol = atol,\n    rtol = rtol,\n  ),\n  :ARCqK => nlp -> ARCqKOp(\n    nlp,\n    max_time = max_time,\n    max_iter = typemax(Int64),\n    max_eval = typemax(Int64),\n    atol = atol,\n    rtol = rtol,\n  ),\n)\n\nstats = bmark_solvers(solvers, cutest_problems)","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"The function bmark_solvers return a Dict of DataFrames with detailed information on the execution. This output can be saved in a data file.","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"using JLD2\n@save \"trunk_arcqk_$(string(length(pnames))).jld2\" stats","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"The result of the benchmark can be explored via tables,","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"pretty_stats(stats[:ARCqK])","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"or it can also be used to make performance profiles.","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"using Plots\ngr()\n\nlegend = Dict(\n  :neval_obj => \"number of f evals\", \n  :neval_cons => \"number of c evals\", \n  :neval_grad => \"number of ∇f evals\", \n  :neval_jac => \"number of ∇c evals\", \n  :neval_jprod => \"number of ∇c*v evals\", \n  :neval_jtprod  => \"number of ∇cᵀ*v evals\", \n  :neval_hess  => \"number of ∇²f evals\",\n  :neval_hprod => \"number of ∇²f*v evals\",\n  :elapsed_time => \"elapsed time\"\n)\nperf_title(col) = \"Performance profile on CUTEst w.r.t. $(string(legend[col]))\"\n\nstyles = [:solid,:dash,:dot,:dashdot] #[:auto, :solid, :dash, :dot, :dashdot, :dashdotdot]\n\nfunction print_pp_column(col::Symbol, stats)\n  \n  ϵ = minimum(minimum(filter(x -> x > 0, df[!, col])) for df in values(stats))\n  first_order(df) = df.status .== :first_order\n  unbounded(df) = df.status .== :unbounded\n  solved(df) = first_order(df) .| unbounded(df)\n  cost(df) = (max.(df[!, col], ϵ) + .!solved(df) .* Inf)\n\n  p = performance_profile(\n    stats, \n    cost, \n    title=perf_title(col), \n    legend=:bottomright, \n    linestyles=styles\n  )\nend\n\nprint_pp_column(:elapsed_time, stats) # with respect to time","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"print_pp_column(:neval_hprod, stats) # with respect to number of Hession-vector products","category":"page"},{"location":"#AdaptiveRegularization-:-A-unified-efficient-implementation-of-trust-region-type-algorithms-for-unconstrained-optimization","page":"Introduction","title":"AdaptiveRegularization : A unified efficient implementation of trust-region type algorithms for unconstrained optimization","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"AdaptiveRegularization is a solver for unconstrained nonlinear problems,","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"min f(x)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"It uses other JuliaSmoothOptimizers packages for development. In particular, NLPModels.jl is used for defining the problem, and SolverCore.jl for the output.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"This package uses Stopping.jl via NLPStopping to handle its workflow, you can also see tutorials with Stopping to learn more.","category":"page"},{"location":"#Algorithm","page":"Introduction","title":"Algorithm","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"The initial implementation of this package follows (Dussault, J.-P. 2020):","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Adaptive cubic regularization (ARC) and trust-region (TR) methods use modified linear systems to compute their steps. The modified systems consist in adding some multiple of the identity matrix (or a well-chosen positive definite matrix) to the Hessian to obtain a sufficiently positive definite linear system, the so called shifted system. This type of system was first proposed by Levenberg and Marquardt. Some trial and error is often involved to obtain a specified value for this shift parameter. We provide an efficient unified implementation to track the shift parameter; our implementation encompasses many ARC and TR variants.","category":"page"},{"location":"#References","page":"Introduction","title":"References","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Dussault, J.-P. (2020). A unified efficient implementation of trust-region type algorithms for unconstrained optimization. INFOR: Information Systems and Operational Research, 58(2), 290-309. 10.1080/03155986.2019.1624490","category":"page"},{"location":"#How-to-Cite","page":"Introduction","title":"How to Cite","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"If you use AdaptiveRegularization.jl in your work, please cite using the format given in CITATION.cff.  <!–https://citation-file-format.github.io/cff-initializer-javascript/#/ –>","category":"page"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"pkg> add https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl","category":"page"},{"location":"#Example","page":"Introduction","title":"Example","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"using AdaptiveRegularization, ADNLPModels\n\n# Rosenbrock\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0])\nstats = ARCqKOp(nlp)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"using AdaptiveRegularization, ADNLPModels, SolverCore\n\n# Rosenbrock\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0])\nsolver = TRARCSolver(nlp)\nstats = GenericExecutionStats(nlp)\nsolve!(solver, nlp, stats, x = [-1.2; 1.0])","category":"page"},{"location":"#Bug-reports-and-discussions","page":"Introduction","title":"Bug reports and discussions","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"If you think you found a bug, feel free to open an issue. Focused suggestions and requests can also be opened as issues. Before opening a pull request, start an issue or a discussion on the topic, please.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"If you want to ask a question not suited for a bug report, feel free to start a discussion here. This forum is for general discussion about this repository and the JuliaSmoothOptimizers, so questions about any of our packages are welcome.","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We show here the basic features of the package.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using AdaptiveRegularization, ADNLPModels\n\n# Rosenbrock\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0])\nstats = ARCqKOp(nlp, verbose = true)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"It is possible to access the number of evaluations of each function of the NLPModel API using the following:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using NLPModels\n\nnobj = neval_obj(nlp) # return number of f call\nngra = neval_grad(nlp) # return number of gradient call\nnhes = neval_hess(nlp) # return number of Hessian call\nnhpr = neval_hprod(nlp) # return number of hessian-vector products\n\n(nobj, ngra, nhes, nhpr)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"These functions come from the NLPModel API defined in NLPModels.jl. If you want to reset the internal counter, you just do reset!(nlp).","category":"page"}]
}
