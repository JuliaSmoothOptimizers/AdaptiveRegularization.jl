<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reference · AdaptiveRegularization.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">AdaptiveRegularization.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><a class="tocitem" href="../benchmark/">Benchmark</a></li><li><a class="tocitem" href="../doityourself/">Do it yourself</a></li><li class="is-active"><a class="tocitem" href>Reference</a><ul class="internal"><li><a class="tocitem" href="#Contents"><span>Contents</span></a></li><li><a class="tocitem" href="#Index"><span>Index</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Reference</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/main/docs/src/reference.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h1><p>​</p><h2 id="Contents"><a class="docs-heading-anchor" href="#Contents">Contents</a><a id="Contents-1"></a><a class="docs-heading-anchor-permalink" href="#Contents" title="Permalink"></a></h2><p>​</p><ul><li><a href="#Reference">Reference</a></li><li class="no-marker"><ul><li><a href="#Contents">Contents</a></li><li><a href="#Index">Index</a></li></ul></li></ul><p>​</p><h2 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h2><p>​</p><ul><li><a href="#AdaptiveRegularization.HessDense"><code>AdaptiveRegularization.HessDense</code></a></li><li><a href="#AdaptiveRegularization.HessGaussNewtonOp"><code>AdaptiveRegularization.HessGaussNewtonOp</code></a></li><li><a href="#AdaptiveRegularization.HessOp"><code>AdaptiveRegularization.HessOp</code></a></li><li><a href="#AdaptiveRegularization.HessSparse"><code>AdaptiveRegularization.HessSparse</code></a></li><li><a href="#AdaptiveRegularization.HessSparseCOO"><code>AdaptiveRegularization.HessSparseCOO</code></a></li><li><a href="#AdaptiveRegularization.PDataKARC"><code>AdaptiveRegularization.PDataKARC</code></a></li><li><a href="#AdaptiveRegularization.PDataNLSST"><code>AdaptiveRegularization.PDataNLSST</code></a></li><li><a href="#AdaptiveRegularization.PDataST"><code>AdaptiveRegularization.PDataST</code></a></li><li><a href="#AdaptiveRegularization.PDataTRK"><code>AdaptiveRegularization.PDataTRK</code></a></li><li><a href="#AdaptiveRegularization.TRARCSolver"><code>AdaptiveRegularization.TRARCSolver</code></a></li><li><a href="#AdaptiveRegularization.TRARCWorkspace"><code>AdaptiveRegularization.TRARCWorkspace</code></a></li><li><a href="#AdaptiveRegularization.ARCqKCOO-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ARCqKCOO</code></a></li><li><a href="#AdaptiveRegularization.ARCqKOp-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ARCqKOp</code></a></li><li><a href="#AdaptiveRegularization.ARCqKOpGN-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ARCqKOpGN</code></a></li><li><a href="#AdaptiveRegularization.ARCqKdense-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ARCqKdense</code></a></li><li><a href="#AdaptiveRegularization.ARCqKsparse-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ARCqKsparse</code></a></li><li><a href="#AdaptiveRegularization.ST_TROp-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ST_TROp</code></a></li><li><a href="#AdaptiveRegularization.ST_TROpGN-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ST_TROpGN</code></a></li><li><a href="#AdaptiveRegularization.ST_TROpGNLSCgls-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ST_TROpGNLSCgls</code></a></li><li><a href="#AdaptiveRegularization.ST_TROpGNLSLsqr-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ST_TROpGNLSLsqr</code></a></li><li><a href="#AdaptiveRegularization.ST_TROpLS-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ST_TROpLS</code></a></li><li><a href="#AdaptiveRegularization.ST_TRdense-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ST_TRdense</code></a></li><li><a href="#AdaptiveRegularization.ST_TRsparse-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ST_TRsparse</code></a></li><li><a href="#AdaptiveRegularization.TRARC"><code>AdaptiveRegularization.TRARC</code></a></li><li><a href="#AdaptiveRegularization.TRKOp-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.TRKOp</code></a></li><li><a href="#AdaptiveRegularization.TRKdense-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.TRKdense</code></a></li><li><a href="#AdaptiveRegularization.TRKsparse-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.TRKsparse</code></a></li><li><a href="#AdaptiveRegularization.compute_Δq-NTuple{4, Any}"><code>AdaptiveRegularization.compute_Δq</code></a></li><li><a href="#AdaptiveRegularization.decrease-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData, T, SolverTools.ARTrustRegion}} where T"><code>AdaptiveRegularization.decrease</code></a></li><li><a href="#AdaptiveRegularization.hessian!-Tuple{AdaptiveRegularization.AbstractHess, Any, Any}"><code>AdaptiveRegularization.hessian!</code></a></li><li><a href="#AdaptiveRegularization.increase-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData, T, SolverTools.ARTrustRegion}} where T"><code>AdaptiveRegularization.increase</code></a></li><li><a href="#AdaptiveRegularization.init-Union{Tuple{S}, Tuple{T}, Tuple{Hess}, Tuple{Type{Hess}, NLPModels.AbstractNLPModel{T, S}, Any}} where {Hess&lt;:AdaptiveRegularization.AbstractHess, T, S}"><code>AdaptiveRegularization.init</code></a></li><li><a href="#AdaptiveRegularization.preprocess!-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData{T}, Vararg{Any, 6}}} where T"><code>AdaptiveRegularization.preprocess!</code></a></li><li><a href="#AdaptiveRegularization.solve_model!-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData{T}, Vararg{Any, 6}}} where T"><code>AdaptiveRegularization.solve_model!</code></a></li></ul><p>​</p><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.HessDense" href="#AdaptiveRegularization.HessDense"><code>AdaptiveRegularization.HessDense</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HessDense(::AbstractNLPModel{T,S}, n)</code></pre><p>Return a structure used for the evaluation of dense Hessian matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/utils/hessian_rep.jl#L7-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.HessGaussNewtonOp" href="#AdaptiveRegularization.HessGaussNewtonOp"><code>AdaptiveRegularization.HessGaussNewtonOp</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HessGaussNewtonOp(::AbstractNLSModel{T,S}, n)</code></pre><p>Return a structure used for the evaluation of the Hessian matrix as an operator.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/utils/hessian_rep.jl#L68-L71">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.HessOp" href="#AdaptiveRegularization.HessOp"><code>AdaptiveRegularization.HessOp</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HessOp(::AbstractNLPModel{T,S}, n)</code></pre><p>Return a structure used for the evaluation of the Hessian matrix as an operator.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/utils/hessian_rep.jl#L51-L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.HessSparse" href="#AdaptiveRegularization.HessSparse"><code>AdaptiveRegularization.HessSparse</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HessSparse(::AbstractNLPModel{T,S}, n)</code></pre><p>Return a structure used for the evaluation of sparse Hessian matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/utils/hessian_rep.jl#L19-L22">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.HessSparseCOO" href="#AdaptiveRegularization.HessSparseCOO"><code>AdaptiveRegularization.HessSparseCOO</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HessSparseCOO(::AbstractNLPModel{T,S}, n)</code></pre><p>Return a structure used for the evaluation of sparse Hessian matrix in COO-format.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/utils/hessian_rep.jl#L36-L39">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.PDataKARC" href="#AdaptiveRegularization.PDataKARC"><code>AdaptiveRegularization.PDataKARC</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PDataKARC(::Type{S}, ::Type{T}, n)</code></pre><p>Return a structure used for the preprocessing of ARCqK methods.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/utils/pdata_struct.jl#L48-L51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.PDataNLSST" href="#AdaptiveRegularization.PDataNLSST"><code>AdaptiveRegularization.PDataNLSST</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PDataNLSST(::Type{S}, ::Type{T}, n)</code></pre><p>Return a structure used for the preprocessing of Steihaug-Toint methods for Gauss-Newton approximation of nonlinear least squares.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/utils/pdata_struct.jl#L227-L230">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.PDataST" href="#AdaptiveRegularization.PDataST"><code>AdaptiveRegularization.PDataST</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PDataST(::Type{S}, ::Type{T}, n)</code></pre><p>Return a structure used for the preprocessing of Steihaug-Toint methods.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/utils/pdata_struct.jl#L190-L193">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.PDataTRK" href="#AdaptiveRegularization.PDataTRK"><code>AdaptiveRegularization.PDataTRK</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PDataTRK(::Type{S}, ::Type{T}, n)</code></pre><p>Return a structure used for the preprocessing of TRK methods.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/utils/pdata_struct.jl#L119-L122">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.TRARCSolver" href="#AdaptiveRegularization.TRARCSolver"><code>AdaptiveRegularization.TRARCSolver</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TRARCSolver(nlp::AbstractNLPModel [, x0 = nlp.meta.x0]; kwargs...)
TRARCSolver(stp::NLPStopping; kwargs...)</code></pre><p>Structure regrouping all the structure used during the <code>TRARC</code> call. It returns a <code>TRARCSolver</code> structure.</p><p><strong>Arguments</strong></p><p>The keyword arguments may include:</p><ul><li><code>stp::NLPStopping</code>: <code>Stopping</code> structure for this algorithm workflow;</li><li><code>meta::ParamData</code>: see <a href="@ref"><code>ParamData</code></a>;</li><li><code>workspace::TRARCWorkspace</code>: allocated space for the solver itself;</li><li><code>TR::ARTrustRegion</code>: trust-region parameters.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/trarc_solver.jl#L79-L93">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.TRARCWorkspace" href="#AdaptiveRegularization.TRARCWorkspace"><code>AdaptiveRegularization.TRARCWorkspace</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TRARCWorkspace(nlp, ::Type{Hess}, n)</code></pre><p>Pre-allocate the memory used during the <a href="#AdaptiveRegularization.TRARC"><code>TRARC</code></a> call for the problem <code>nlp</code> of size <code>n</code>. The possible values for <code>Hess</code> are: <code>HessDense</code>, <code>HessSparse</code>, <code>HessSparseCOO</code>, <code>HessOp</code>. Return a <code>TRARCWorkspace</code> structure.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/trarc_solver.jl#L1-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.ARCqKCOO-Tuple{Stopping.NLPStopping}" href="#AdaptiveRegularization.ARCqKCOO-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ARCqKCOO</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">TRARC(nlp; kwargs...)</code></pre><p>Compute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.</p><p>Some variants of TRARC are already implemented and listed in <code>AdaptiveRegularization.ALL_solvers</code>.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel</code>: the model solved, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments include</p><ul><li><code>TR::ARTrustRegion</code>: structure with trust-region/ARC parameters, see <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl"><code>SolverTools.jl</code></a>. Default: <code>ARTrustRegion(T(10.0))</code>.</li><li><code>hess_type::Type{Hess}</code>: Structure used to handle the hessian. The possible values are: <code>HessDense</code>, <code>HessSparse</code>, <code>HessSparseCOO</code>, <code>HessOp</code>. Default: <code>HessOp</code>.</li><li><code>pdata_type::Type{ParamData}</code> Structure used for the preprocessing step. Default: <code>PDataKARC</code>.</li><li><code>robust::Bool</code>: <code>true</code> implements a robust evaluation of the model. Default: <code>true</code>.</li><li><code>verbose::Bool</code>: <code>true</code> prints iteration information. Default: <code>false</code>.</li></ul><p>Additional <code>kwargs</code> are used for stopping criterion, see <code>Stopping.jl</code>.</p><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.stp</code>: stopping object used for the algorithm;</li><li><code>solver.workspace</code>: additional allocations;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p>This implementation uses <code>Stopping.jl</code>. Therefore, it is also possible to used</p><pre><code class="nohighlight hljs">TRARC(stp; kwargs...)</code></pre><p>which returns the <code>stp::NLPStopping</code> updated.</p><p>For advanced usage, first define a <a href="#AdaptiveRegularization.TRARCSolver"><code>TRARCSolver</code></a> to preallocate the  memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">stats = solve!(solver, nlp)
stats = solve!(solver, nlp, stats)</code></pre><p>To choose a particular variant, the keyword arguments <code>hess_type</code> and <code>pdata_type</code> can be used as follows:    <code>TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC)</code> the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.</p><p><strong>References</strong></p><p>This method unifies the implementation of trust-region and adaptive regularization with cubics as described in</p><pre><code class="nohighlight hljs">Dussault, J.-P. (2020).
A unified efficient implementation of trust-region type algorithms for unconstrained optimization.
INFOR: Information Systems and Operational Research, 58(2), 290-309.
10.1080/03155986.2019.1624490</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
stats = TRARC(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = solve!(solver, nlp)</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = GenericExecutionStats(nlp)
stats = solve!(solver, nlp, stats)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/AdaptiveRegularization.jl#L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.ARCqKOp-Tuple{Stopping.NLPStopping}" href="#AdaptiveRegularization.ARCqKOp-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ARCqKOp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">TRARC(nlp; kwargs...)</code></pre><p>Compute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.</p><p>Some variants of TRARC are already implemented and listed in <code>AdaptiveRegularization.ALL_solvers</code>.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel</code>: the model solved, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments include</p><ul><li><code>TR::ARTrustRegion</code>: structure with trust-region/ARC parameters, see <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl"><code>SolverTools.jl</code></a>. Default: <code>ARTrustRegion(T(10.0))</code>.</li><li><code>hess_type::Type{Hess}</code>: Structure used to handle the hessian. The possible values are: <code>HessDense</code>, <code>HessSparse</code>, <code>HessSparseCOO</code>, <code>HessOp</code>. Default: <code>HessOp</code>.</li><li><code>pdata_type::Type{ParamData}</code> Structure used for the preprocessing step. Default: <code>PDataKARC</code>.</li><li><code>robust::Bool</code>: <code>true</code> implements a robust evaluation of the model. Default: <code>true</code>.</li><li><code>verbose::Bool</code>: <code>true</code> prints iteration information. Default: <code>false</code>.</li></ul><p>Additional <code>kwargs</code> are used for stopping criterion, see <code>Stopping.jl</code>.</p><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.stp</code>: stopping object used for the algorithm;</li><li><code>solver.workspace</code>: additional allocations;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p>This implementation uses <code>Stopping.jl</code>. Therefore, it is also possible to used</p><pre><code class="nohighlight hljs">TRARC(stp; kwargs...)</code></pre><p>which returns the <code>stp::NLPStopping</code> updated.</p><p>For advanced usage, first define a <a href="#AdaptiveRegularization.TRARCSolver"><code>TRARCSolver</code></a> to preallocate the  memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">stats = solve!(solver, nlp)
stats = solve!(solver, nlp, stats)</code></pre><p>To choose a particular variant, the keyword arguments <code>hess_type</code> and <code>pdata_type</code> can be used as follows:    <code>TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC)</code> the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.</p><p><strong>References</strong></p><p>This method unifies the implementation of trust-region and adaptive regularization with cubics as described in</p><pre><code class="nohighlight hljs">Dussault, J.-P. (2020).
A unified efficient implementation of trust-region type algorithms for unconstrained optimization.
INFOR: Information Systems and Operational Research, 58(2), 290-309.
10.1080/03155986.2019.1624490</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
stats = TRARC(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = solve!(solver, nlp)</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = GenericExecutionStats(nlp)
stats = solve!(solver, nlp, stats)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/AdaptiveRegularization.jl#L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.ARCqKOpGN-Tuple{Stopping.NLPStopping}" href="#AdaptiveRegularization.ARCqKOpGN-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ARCqKOpGN</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">TRARC(nlp; kwargs...)</code></pre><p>Compute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.</p><p>Some variants of TRARC are already implemented and listed in <code>AdaptiveRegularization.ALL_solvers</code>.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel</code>: the model solved, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments include</p><ul><li><code>TR::ARTrustRegion</code>: structure with trust-region/ARC parameters, see <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl"><code>SolverTools.jl</code></a>. Default: <code>ARTrustRegion(T(10.0))</code>.</li><li><code>hess_type::Type{Hess}</code>: Structure used to handle the hessian. The possible values are: <code>HessDense</code>, <code>HessSparse</code>, <code>HessSparseCOO</code>, <code>HessOp</code>. Default: <code>HessOp</code>.</li><li><code>pdata_type::Type{ParamData}</code> Structure used for the preprocessing step. Default: <code>PDataKARC</code>.</li><li><code>robust::Bool</code>: <code>true</code> implements a robust evaluation of the model. Default: <code>true</code>.</li><li><code>verbose::Bool</code>: <code>true</code> prints iteration information. Default: <code>false</code>.</li></ul><p>Additional <code>kwargs</code> are used for stopping criterion, see <code>Stopping.jl</code>.</p><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.stp</code>: stopping object used for the algorithm;</li><li><code>solver.workspace</code>: additional allocations;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p>This implementation uses <code>Stopping.jl</code>. Therefore, it is also possible to used</p><pre><code class="nohighlight hljs">TRARC(stp; kwargs...)</code></pre><p>which returns the <code>stp::NLPStopping</code> updated.</p><p>For advanced usage, first define a <a href="#AdaptiveRegularization.TRARCSolver"><code>TRARCSolver</code></a> to preallocate the  memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">stats = solve!(solver, nlp)
stats = solve!(solver, nlp, stats)</code></pre><p>To choose a particular variant, the keyword arguments <code>hess_type</code> and <code>pdata_type</code> can be used as follows:    <code>TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC)</code> the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.</p><p><strong>References</strong></p><p>This method unifies the implementation of trust-region and adaptive regularization with cubics as described in</p><pre><code class="nohighlight hljs">Dussault, J.-P. (2020).
A unified efficient implementation of trust-region type algorithms for unconstrained optimization.
INFOR: Information Systems and Operational Research, 58(2), 290-309.
10.1080/03155986.2019.1624490</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
stats = TRARC(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = solve!(solver, nlp)</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = GenericExecutionStats(nlp)
stats = solve!(solver, nlp, stats)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/AdaptiveRegularization.jl#L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.ARCqKdense-Tuple{Stopping.NLPStopping}" href="#AdaptiveRegularization.ARCqKdense-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ARCqKdense</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">TRARC(nlp; kwargs...)</code></pre><p>Compute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.</p><p>Some variants of TRARC are already implemented and listed in <code>AdaptiveRegularization.ALL_solvers</code>.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel</code>: the model solved, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments include</p><ul><li><code>TR::ARTrustRegion</code>: structure with trust-region/ARC parameters, see <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl"><code>SolverTools.jl</code></a>. Default: <code>ARTrustRegion(T(10.0))</code>.</li><li><code>hess_type::Type{Hess}</code>: Structure used to handle the hessian. The possible values are: <code>HessDense</code>, <code>HessSparse</code>, <code>HessSparseCOO</code>, <code>HessOp</code>. Default: <code>HessOp</code>.</li><li><code>pdata_type::Type{ParamData}</code> Structure used for the preprocessing step. Default: <code>PDataKARC</code>.</li><li><code>robust::Bool</code>: <code>true</code> implements a robust evaluation of the model. Default: <code>true</code>.</li><li><code>verbose::Bool</code>: <code>true</code> prints iteration information. Default: <code>false</code>.</li></ul><p>Additional <code>kwargs</code> are used for stopping criterion, see <code>Stopping.jl</code>.</p><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.stp</code>: stopping object used for the algorithm;</li><li><code>solver.workspace</code>: additional allocations;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p>This implementation uses <code>Stopping.jl</code>. Therefore, it is also possible to used</p><pre><code class="nohighlight hljs">TRARC(stp; kwargs...)</code></pre><p>which returns the <code>stp::NLPStopping</code> updated.</p><p>For advanced usage, first define a <a href="#AdaptiveRegularization.TRARCSolver"><code>TRARCSolver</code></a> to preallocate the  memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">stats = solve!(solver, nlp)
stats = solve!(solver, nlp, stats)</code></pre><p>To choose a particular variant, the keyword arguments <code>hess_type</code> and <code>pdata_type</code> can be used as follows:    <code>TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC)</code> the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.</p><p><strong>References</strong></p><p>This method unifies the implementation of trust-region and adaptive regularization with cubics as described in</p><pre><code class="nohighlight hljs">Dussault, J.-P. (2020).
A unified efficient implementation of trust-region type algorithms for unconstrained optimization.
INFOR: Information Systems and Operational Research, 58(2), 290-309.
10.1080/03155986.2019.1624490</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
stats = TRARC(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = solve!(solver, nlp)</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = GenericExecutionStats(nlp)
stats = solve!(solver, nlp, stats)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/AdaptiveRegularization.jl#L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.ARCqKsparse-Tuple{Stopping.NLPStopping}" href="#AdaptiveRegularization.ARCqKsparse-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ARCqKsparse</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">TRARC(nlp; kwargs...)</code></pre><p>Compute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.</p><p>Some variants of TRARC are already implemented and listed in <code>AdaptiveRegularization.ALL_solvers</code>.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel</code>: the model solved, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments include</p><ul><li><code>TR::ARTrustRegion</code>: structure with trust-region/ARC parameters, see <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl"><code>SolverTools.jl</code></a>. Default: <code>ARTrustRegion(T(10.0))</code>.</li><li><code>hess_type::Type{Hess}</code>: Structure used to handle the hessian. The possible values are: <code>HessDense</code>, <code>HessSparse</code>, <code>HessSparseCOO</code>, <code>HessOp</code>. Default: <code>HessOp</code>.</li><li><code>pdata_type::Type{ParamData}</code> Structure used for the preprocessing step. Default: <code>PDataKARC</code>.</li><li><code>robust::Bool</code>: <code>true</code> implements a robust evaluation of the model. Default: <code>true</code>.</li><li><code>verbose::Bool</code>: <code>true</code> prints iteration information. Default: <code>false</code>.</li></ul><p>Additional <code>kwargs</code> are used for stopping criterion, see <code>Stopping.jl</code>.</p><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.stp</code>: stopping object used for the algorithm;</li><li><code>solver.workspace</code>: additional allocations;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p>This implementation uses <code>Stopping.jl</code>. Therefore, it is also possible to used</p><pre><code class="nohighlight hljs">TRARC(stp; kwargs...)</code></pre><p>which returns the <code>stp::NLPStopping</code> updated.</p><p>For advanced usage, first define a <a href="#AdaptiveRegularization.TRARCSolver"><code>TRARCSolver</code></a> to preallocate the  memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">stats = solve!(solver, nlp)
stats = solve!(solver, nlp, stats)</code></pre><p>To choose a particular variant, the keyword arguments <code>hess_type</code> and <code>pdata_type</code> can be used as follows:    <code>TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC)</code> the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.</p><p><strong>References</strong></p><p>This method unifies the implementation of trust-region and adaptive regularization with cubics as described in</p><pre><code class="nohighlight hljs">Dussault, J.-P. (2020).
A unified efficient implementation of trust-region type algorithms for unconstrained optimization.
INFOR: Information Systems and Operational Research, 58(2), 290-309.
10.1080/03155986.2019.1624490</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
stats = TRARC(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = solve!(solver, nlp)</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = GenericExecutionStats(nlp)
stats = solve!(solver, nlp, stats)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/AdaptiveRegularization.jl#L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.ST_TROp-Tuple{Stopping.NLPStopping}" href="#AdaptiveRegularization.ST_TROp-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ST_TROp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">TRARC(nlp; kwargs...)</code></pre><p>Compute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.</p><p>Some variants of TRARC are already implemented and listed in <code>AdaptiveRegularization.ALL_solvers</code>.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel</code>: the model solved, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments include</p><ul><li><code>TR::ARTrustRegion</code>: structure with trust-region/ARC parameters, see <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl"><code>SolverTools.jl</code></a>. Default: <code>ARTrustRegion(T(10.0))</code>.</li><li><code>hess_type::Type{Hess}</code>: Structure used to handle the hessian. The possible values are: <code>HessDense</code>, <code>HessSparse</code>, <code>HessSparseCOO</code>, <code>HessOp</code>. Default: <code>HessOp</code>.</li><li><code>pdata_type::Type{ParamData}</code> Structure used for the preprocessing step. Default: <code>PDataKARC</code>.</li><li><code>robust::Bool</code>: <code>true</code> implements a robust evaluation of the model. Default: <code>true</code>.</li><li><code>verbose::Bool</code>: <code>true</code> prints iteration information. Default: <code>false</code>.</li></ul><p>Additional <code>kwargs</code> are used for stopping criterion, see <code>Stopping.jl</code>.</p><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.stp</code>: stopping object used for the algorithm;</li><li><code>solver.workspace</code>: additional allocations;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p>This implementation uses <code>Stopping.jl</code>. Therefore, it is also possible to used</p><pre><code class="nohighlight hljs">TRARC(stp; kwargs...)</code></pre><p>which returns the <code>stp::NLPStopping</code> updated.</p><p>For advanced usage, first define a <a href="#AdaptiveRegularization.TRARCSolver"><code>TRARCSolver</code></a> to preallocate the  memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">stats = solve!(solver, nlp)
stats = solve!(solver, nlp, stats)</code></pre><p>To choose a particular variant, the keyword arguments <code>hess_type</code> and <code>pdata_type</code> can be used as follows:    <code>TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC)</code> the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.</p><p><strong>References</strong></p><p>This method unifies the implementation of trust-region and adaptive regularization with cubics as described in</p><pre><code class="nohighlight hljs">Dussault, J.-P. (2020).
A unified efficient implementation of trust-region type algorithms for unconstrained optimization.
INFOR: Information Systems and Operational Research, 58(2), 290-309.
10.1080/03155986.2019.1624490</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
stats = TRARC(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = solve!(solver, nlp)</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = GenericExecutionStats(nlp)
stats = solve!(solver, nlp, stats)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/AdaptiveRegularization.jl#L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.ST_TROpGN-Tuple{Stopping.NLPStopping}" href="#AdaptiveRegularization.ST_TROpGN-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ST_TROpGN</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">TRARC(nlp; kwargs...)</code></pre><p>Compute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.</p><p>Some variants of TRARC are already implemented and listed in <code>AdaptiveRegularization.ALL_solvers</code>.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel</code>: the model solved, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments include</p><ul><li><code>TR::ARTrustRegion</code>: structure with trust-region/ARC parameters, see <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl"><code>SolverTools.jl</code></a>. Default: <code>ARTrustRegion(T(10.0))</code>.</li><li><code>hess_type::Type{Hess}</code>: Structure used to handle the hessian. The possible values are: <code>HessDense</code>, <code>HessSparse</code>, <code>HessSparseCOO</code>, <code>HessOp</code>. Default: <code>HessOp</code>.</li><li><code>pdata_type::Type{ParamData}</code> Structure used for the preprocessing step. Default: <code>PDataKARC</code>.</li><li><code>robust::Bool</code>: <code>true</code> implements a robust evaluation of the model. Default: <code>true</code>.</li><li><code>verbose::Bool</code>: <code>true</code> prints iteration information. Default: <code>false</code>.</li></ul><p>Additional <code>kwargs</code> are used for stopping criterion, see <code>Stopping.jl</code>.</p><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.stp</code>: stopping object used for the algorithm;</li><li><code>solver.workspace</code>: additional allocations;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p>This implementation uses <code>Stopping.jl</code>. Therefore, it is also possible to used</p><pre><code class="nohighlight hljs">TRARC(stp; kwargs...)</code></pre><p>which returns the <code>stp::NLPStopping</code> updated.</p><p>For advanced usage, first define a <a href="#AdaptiveRegularization.TRARCSolver"><code>TRARCSolver</code></a> to preallocate the  memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">stats = solve!(solver, nlp)
stats = solve!(solver, nlp, stats)</code></pre><p>To choose a particular variant, the keyword arguments <code>hess_type</code> and <code>pdata_type</code> can be used as follows:    <code>TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC)</code> the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.</p><p><strong>References</strong></p><p>This method unifies the implementation of trust-region and adaptive regularization with cubics as described in</p><pre><code class="nohighlight hljs">Dussault, J.-P. (2020).
A unified efficient implementation of trust-region type algorithms for unconstrained optimization.
INFOR: Information Systems and Operational Research, 58(2), 290-309.
10.1080/03155986.2019.1624490</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
stats = TRARC(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = solve!(solver, nlp)</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = GenericExecutionStats(nlp)
stats = solve!(solver, nlp, stats)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/AdaptiveRegularization.jl#L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.ST_TROpGNLSCgls-Tuple{Stopping.NLPStopping}" href="#AdaptiveRegularization.ST_TROpGNLSCgls-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ST_TROpGNLSCgls</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">TRARC(nlp; kwargs...)</code></pre><p>Compute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.</p><p>Some variants of TRARC are already implemented and listed in <code>AdaptiveRegularization.ALL_solvers</code>.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel</code>: the model solved, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments include</p><ul><li><code>TR::ARTrustRegion</code>: structure with trust-region/ARC parameters, see <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl"><code>SolverTools.jl</code></a>. Default: <code>ARTrustRegion(T(10.0))</code>.</li><li><code>hess_type::Type{Hess}</code>: Structure used to handle the hessian. The possible values are: <code>HessDense</code>, <code>HessSparse</code>, <code>HessSparseCOO</code>, <code>HessOp</code>. Default: <code>HessOp</code>.</li><li><code>pdata_type::Type{ParamData}</code> Structure used for the preprocessing step. Default: <code>PDataKARC</code>.</li><li><code>robust::Bool</code>: <code>true</code> implements a robust evaluation of the model. Default: <code>true</code>.</li><li><code>verbose::Bool</code>: <code>true</code> prints iteration information. Default: <code>false</code>.</li></ul><p>Additional <code>kwargs</code> are used for stopping criterion, see <code>Stopping.jl</code>.</p><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.stp</code>: stopping object used for the algorithm;</li><li><code>solver.workspace</code>: additional allocations;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p>This implementation uses <code>Stopping.jl</code>. Therefore, it is also possible to used</p><pre><code class="nohighlight hljs">TRARC(stp; kwargs...)</code></pre><p>which returns the <code>stp::NLPStopping</code> updated.</p><p>For advanced usage, first define a <a href="#AdaptiveRegularization.TRARCSolver"><code>TRARCSolver</code></a> to preallocate the  memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">stats = solve!(solver, nlp)
stats = solve!(solver, nlp, stats)</code></pre><p>To choose a particular variant, the keyword arguments <code>hess_type</code> and <code>pdata_type</code> can be used as follows:    <code>TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC)</code> the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.</p><p><strong>References</strong></p><p>This method unifies the implementation of trust-region and adaptive regularization with cubics as described in</p><pre><code class="nohighlight hljs">Dussault, J.-P. (2020).
A unified efficient implementation of trust-region type algorithms for unconstrained optimization.
INFOR: Information Systems and Operational Research, 58(2), 290-309.
10.1080/03155986.2019.1624490</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
stats = TRARC(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = solve!(solver, nlp)</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = GenericExecutionStats(nlp)
stats = solve!(solver, nlp, stats)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/AdaptiveRegularization.jl#L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.ST_TROpGNLSLsqr-Tuple{Stopping.NLPStopping}" href="#AdaptiveRegularization.ST_TROpGNLSLsqr-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ST_TROpGNLSLsqr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">TRARC(nlp; kwargs...)</code></pre><p>Compute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.</p><p>Some variants of TRARC are already implemented and listed in <code>AdaptiveRegularization.ALL_solvers</code>.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel</code>: the model solved, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments include</p><ul><li><code>TR::ARTrustRegion</code>: structure with trust-region/ARC parameters, see <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl"><code>SolverTools.jl</code></a>. Default: <code>ARTrustRegion(T(10.0))</code>.</li><li><code>hess_type::Type{Hess}</code>: Structure used to handle the hessian. The possible values are: <code>HessDense</code>, <code>HessSparse</code>, <code>HessSparseCOO</code>, <code>HessOp</code>. Default: <code>HessOp</code>.</li><li><code>pdata_type::Type{ParamData}</code> Structure used for the preprocessing step. Default: <code>PDataKARC</code>.</li><li><code>robust::Bool</code>: <code>true</code> implements a robust evaluation of the model. Default: <code>true</code>.</li><li><code>verbose::Bool</code>: <code>true</code> prints iteration information. Default: <code>false</code>.</li></ul><p>Additional <code>kwargs</code> are used for stopping criterion, see <code>Stopping.jl</code>.</p><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.stp</code>: stopping object used for the algorithm;</li><li><code>solver.workspace</code>: additional allocations;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p>This implementation uses <code>Stopping.jl</code>. Therefore, it is also possible to used</p><pre><code class="nohighlight hljs">TRARC(stp; kwargs...)</code></pre><p>which returns the <code>stp::NLPStopping</code> updated.</p><p>For advanced usage, first define a <a href="#AdaptiveRegularization.TRARCSolver"><code>TRARCSolver</code></a> to preallocate the  memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">stats = solve!(solver, nlp)
stats = solve!(solver, nlp, stats)</code></pre><p>To choose a particular variant, the keyword arguments <code>hess_type</code> and <code>pdata_type</code> can be used as follows:    <code>TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC)</code> the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.</p><p><strong>References</strong></p><p>This method unifies the implementation of trust-region and adaptive regularization with cubics as described in</p><pre><code class="nohighlight hljs">Dussault, J.-P. (2020).
A unified efficient implementation of trust-region type algorithms for unconstrained optimization.
INFOR: Information Systems and Operational Research, 58(2), 290-309.
10.1080/03155986.2019.1624490</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
stats = TRARC(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = solve!(solver, nlp)</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = GenericExecutionStats(nlp)
stats = solve!(solver, nlp, stats)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/AdaptiveRegularization.jl#L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.ST_TROpLS-Tuple{Stopping.NLPStopping}" href="#AdaptiveRegularization.ST_TROpLS-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ST_TROpLS</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">TRARC(nlp; kwargs...)</code></pre><p>Compute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.</p><p>Some variants of TRARC are already implemented and listed in <code>AdaptiveRegularization.ALL_solvers</code>.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel</code>: the model solved, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments include</p><ul><li><code>TR::ARTrustRegion</code>: structure with trust-region/ARC parameters, see <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl"><code>SolverTools.jl</code></a>. Default: <code>ARTrustRegion(T(10.0))</code>.</li><li><code>hess_type::Type{Hess}</code>: Structure used to handle the hessian. The possible values are: <code>HessDense</code>, <code>HessSparse</code>, <code>HessSparseCOO</code>, <code>HessOp</code>. Default: <code>HessOp</code>.</li><li><code>pdata_type::Type{ParamData}</code> Structure used for the preprocessing step. Default: <code>PDataKARC</code>.</li><li><code>robust::Bool</code>: <code>true</code> implements a robust evaluation of the model. Default: <code>true</code>.</li><li><code>verbose::Bool</code>: <code>true</code> prints iteration information. Default: <code>false</code>.</li></ul><p>Additional <code>kwargs</code> are used for stopping criterion, see <code>Stopping.jl</code>.</p><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.stp</code>: stopping object used for the algorithm;</li><li><code>solver.workspace</code>: additional allocations;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p>This implementation uses <code>Stopping.jl</code>. Therefore, it is also possible to used</p><pre><code class="nohighlight hljs">TRARC(stp; kwargs...)</code></pre><p>which returns the <code>stp::NLPStopping</code> updated.</p><p>For advanced usage, first define a <a href="#AdaptiveRegularization.TRARCSolver"><code>TRARCSolver</code></a> to preallocate the  memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">stats = solve!(solver, nlp)
stats = solve!(solver, nlp, stats)</code></pre><p>To choose a particular variant, the keyword arguments <code>hess_type</code> and <code>pdata_type</code> can be used as follows:    <code>TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC)</code> the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.</p><p><strong>References</strong></p><p>This method unifies the implementation of trust-region and adaptive regularization with cubics as described in</p><pre><code class="nohighlight hljs">Dussault, J.-P. (2020).
A unified efficient implementation of trust-region type algorithms for unconstrained optimization.
INFOR: Information Systems and Operational Research, 58(2), 290-309.
10.1080/03155986.2019.1624490</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
stats = TRARC(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = solve!(solver, nlp)</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = GenericExecutionStats(nlp)
stats = solve!(solver, nlp, stats)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/AdaptiveRegularization.jl#L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.ST_TRdense-Tuple{Stopping.NLPStopping}" href="#AdaptiveRegularization.ST_TRdense-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ST_TRdense</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">TRARC(nlp; kwargs...)</code></pre><p>Compute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.</p><p>Some variants of TRARC are already implemented and listed in <code>AdaptiveRegularization.ALL_solvers</code>.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel</code>: the model solved, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments include</p><ul><li><code>TR::ARTrustRegion</code>: structure with trust-region/ARC parameters, see <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl"><code>SolverTools.jl</code></a>. Default: <code>ARTrustRegion(T(10.0))</code>.</li><li><code>hess_type::Type{Hess}</code>: Structure used to handle the hessian. The possible values are: <code>HessDense</code>, <code>HessSparse</code>, <code>HessSparseCOO</code>, <code>HessOp</code>. Default: <code>HessOp</code>.</li><li><code>pdata_type::Type{ParamData}</code> Structure used for the preprocessing step. Default: <code>PDataKARC</code>.</li><li><code>robust::Bool</code>: <code>true</code> implements a robust evaluation of the model. Default: <code>true</code>.</li><li><code>verbose::Bool</code>: <code>true</code> prints iteration information. Default: <code>false</code>.</li></ul><p>Additional <code>kwargs</code> are used for stopping criterion, see <code>Stopping.jl</code>.</p><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.stp</code>: stopping object used for the algorithm;</li><li><code>solver.workspace</code>: additional allocations;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p>This implementation uses <code>Stopping.jl</code>. Therefore, it is also possible to used</p><pre><code class="nohighlight hljs">TRARC(stp; kwargs...)</code></pre><p>which returns the <code>stp::NLPStopping</code> updated.</p><p>For advanced usage, first define a <a href="#AdaptiveRegularization.TRARCSolver"><code>TRARCSolver</code></a> to preallocate the  memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">stats = solve!(solver, nlp)
stats = solve!(solver, nlp, stats)</code></pre><p>To choose a particular variant, the keyword arguments <code>hess_type</code> and <code>pdata_type</code> can be used as follows:    <code>TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC)</code> the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.</p><p><strong>References</strong></p><p>This method unifies the implementation of trust-region and adaptive regularization with cubics as described in</p><pre><code class="nohighlight hljs">Dussault, J.-P. (2020).
A unified efficient implementation of trust-region type algorithms for unconstrained optimization.
INFOR: Information Systems and Operational Research, 58(2), 290-309.
10.1080/03155986.2019.1624490</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
stats = TRARC(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = solve!(solver, nlp)</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = GenericExecutionStats(nlp)
stats = solve!(solver, nlp, stats)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/AdaptiveRegularization.jl#L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.ST_TRsparse-Tuple{Stopping.NLPStopping}" href="#AdaptiveRegularization.ST_TRsparse-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.ST_TRsparse</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">TRARC(nlp; kwargs...)</code></pre><p>Compute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.</p><p>Some variants of TRARC are already implemented and listed in <code>AdaptiveRegularization.ALL_solvers</code>.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel</code>: the model solved, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments include</p><ul><li><code>TR::ARTrustRegion</code>: structure with trust-region/ARC parameters, see <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl"><code>SolverTools.jl</code></a>. Default: <code>ARTrustRegion(T(10.0))</code>.</li><li><code>hess_type::Type{Hess}</code>: Structure used to handle the hessian. The possible values are: <code>HessDense</code>, <code>HessSparse</code>, <code>HessSparseCOO</code>, <code>HessOp</code>. Default: <code>HessOp</code>.</li><li><code>pdata_type::Type{ParamData}</code> Structure used for the preprocessing step. Default: <code>PDataKARC</code>.</li><li><code>robust::Bool</code>: <code>true</code> implements a robust evaluation of the model. Default: <code>true</code>.</li><li><code>verbose::Bool</code>: <code>true</code> prints iteration information. Default: <code>false</code>.</li></ul><p>Additional <code>kwargs</code> are used for stopping criterion, see <code>Stopping.jl</code>.</p><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.stp</code>: stopping object used for the algorithm;</li><li><code>solver.workspace</code>: additional allocations;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p>This implementation uses <code>Stopping.jl</code>. Therefore, it is also possible to used</p><pre><code class="nohighlight hljs">TRARC(stp; kwargs...)</code></pre><p>which returns the <code>stp::NLPStopping</code> updated.</p><p>For advanced usage, first define a <a href="#AdaptiveRegularization.TRARCSolver"><code>TRARCSolver</code></a> to preallocate the  memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">stats = solve!(solver, nlp)
stats = solve!(solver, nlp, stats)</code></pre><p>To choose a particular variant, the keyword arguments <code>hess_type</code> and <code>pdata_type</code> can be used as follows:    <code>TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC)</code> the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.</p><p><strong>References</strong></p><p>This method unifies the implementation of trust-region and adaptive regularization with cubics as described in</p><pre><code class="nohighlight hljs">Dussault, J.-P. (2020).
A unified efficient implementation of trust-region type algorithms for unconstrained optimization.
INFOR: Information Systems and Operational Research, 58(2), 290-309.
10.1080/03155986.2019.1624490</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
stats = TRARC(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = solve!(solver, nlp)</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = GenericExecutionStats(nlp)
stats = solve!(solver, nlp, stats)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/AdaptiveRegularization.jl#L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.TRARC" href="#AdaptiveRegularization.TRARC"><code>AdaptiveRegularization.TRARC</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">TRARC(nlp; kwargs...)</code></pre><p>Compute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.</p><p>Some variants of TRARC are already implemented and listed in <code>AdaptiveRegularization.ALL_solvers</code>.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel</code>: the model solved, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments include</p><ul><li><code>TR::ARTrustRegion</code>: structure with trust-region/ARC parameters, see <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl"><code>SolverTools.jl</code></a>. Default: <code>ARTrustRegion(T(10.0))</code>.</li><li><code>hess_type::Type{Hess}</code>: Structure used to handle the hessian. The possible values are: <code>HessDense</code>, <code>HessSparse</code>, <code>HessSparseCOO</code>, <code>HessOp</code>. Default: <code>HessOp</code>.</li><li><code>pdata_type::Type{ParamData}</code> Structure used for the preprocessing step. Default: <code>PDataKARC</code>.</li><li><code>robust::Bool</code>: <code>true</code> implements a robust evaluation of the model. Default: <code>true</code>.</li><li><code>verbose::Bool</code>: <code>true</code> prints iteration information. Default: <code>false</code>.</li></ul><p>Additional <code>kwargs</code> are used for stopping criterion, see <code>Stopping.jl</code>.</p><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.stp</code>: stopping object used for the algorithm;</li><li><code>solver.workspace</code>: additional allocations;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p>This implementation uses <code>Stopping.jl</code>. Therefore, it is also possible to used</p><pre><code class="nohighlight hljs">TRARC(stp; kwargs...)</code></pre><p>which returns the <code>stp::NLPStopping</code> updated.</p><p>For advanced usage, first define a <a href="#AdaptiveRegularization.TRARCSolver"><code>TRARCSolver</code></a> to preallocate the  memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">stats = solve!(solver, nlp)
stats = solve!(solver, nlp, stats)</code></pre><p>To choose a particular variant, the keyword arguments <code>hess_type</code> and <code>pdata_type</code> can be used as follows:    <code>TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC)</code> the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.</p><p><strong>References</strong></p><p>This method unifies the implementation of trust-region and adaptive regularization with cubics as described in</p><pre><code class="nohighlight hljs">Dussault, J.-P. (2020).
A unified efficient implementation of trust-region type algorithms for unconstrained optimization.
INFOR: Information Systems and Operational Research, 58(2), 290-309.
10.1080/03155986.2019.1624490</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
stats = TRARC(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = solve!(solver, nlp)</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = GenericExecutionStats(nlp)
stats = solve!(solver, nlp, stats)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/AdaptiveRegularization.jl#L38-L136">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.TRKOp-Tuple{Stopping.NLPStopping}" href="#AdaptiveRegularization.TRKOp-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.TRKOp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">TRARC(nlp; kwargs...)</code></pre><p>Compute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.</p><p>Some variants of TRARC are already implemented and listed in <code>AdaptiveRegularization.ALL_solvers</code>.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel</code>: the model solved, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments include</p><ul><li><code>TR::ARTrustRegion</code>: structure with trust-region/ARC parameters, see <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl"><code>SolverTools.jl</code></a>. Default: <code>ARTrustRegion(T(10.0))</code>.</li><li><code>hess_type::Type{Hess}</code>: Structure used to handle the hessian. The possible values are: <code>HessDense</code>, <code>HessSparse</code>, <code>HessSparseCOO</code>, <code>HessOp</code>. Default: <code>HessOp</code>.</li><li><code>pdata_type::Type{ParamData}</code> Structure used for the preprocessing step. Default: <code>PDataKARC</code>.</li><li><code>robust::Bool</code>: <code>true</code> implements a robust evaluation of the model. Default: <code>true</code>.</li><li><code>verbose::Bool</code>: <code>true</code> prints iteration information. Default: <code>false</code>.</li></ul><p>Additional <code>kwargs</code> are used for stopping criterion, see <code>Stopping.jl</code>.</p><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.stp</code>: stopping object used for the algorithm;</li><li><code>solver.workspace</code>: additional allocations;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p>This implementation uses <code>Stopping.jl</code>. Therefore, it is also possible to used</p><pre><code class="nohighlight hljs">TRARC(stp; kwargs...)</code></pre><p>which returns the <code>stp::NLPStopping</code> updated.</p><p>For advanced usage, first define a <a href="#AdaptiveRegularization.TRARCSolver"><code>TRARCSolver</code></a> to preallocate the  memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">stats = solve!(solver, nlp)
stats = solve!(solver, nlp, stats)</code></pre><p>To choose a particular variant, the keyword arguments <code>hess_type</code> and <code>pdata_type</code> can be used as follows:    <code>TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC)</code> the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.</p><p><strong>References</strong></p><p>This method unifies the implementation of trust-region and adaptive regularization with cubics as described in</p><pre><code class="nohighlight hljs">Dussault, J.-P. (2020).
A unified efficient implementation of trust-region type algorithms for unconstrained optimization.
INFOR: Information Systems and Operational Research, 58(2), 290-309.
10.1080/03155986.2019.1624490</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
stats = TRARC(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = solve!(solver, nlp)</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = GenericExecutionStats(nlp)
stats = solve!(solver, nlp, stats)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/AdaptiveRegularization.jl#L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.TRKdense-Tuple{Stopping.NLPStopping}" href="#AdaptiveRegularization.TRKdense-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.TRKdense</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">TRARC(nlp; kwargs...)</code></pre><p>Compute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.</p><p>Some variants of TRARC are already implemented and listed in <code>AdaptiveRegularization.ALL_solvers</code>.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel</code>: the model solved, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments include</p><ul><li><code>TR::ARTrustRegion</code>: structure with trust-region/ARC parameters, see <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl"><code>SolverTools.jl</code></a>. Default: <code>ARTrustRegion(T(10.0))</code>.</li><li><code>hess_type::Type{Hess}</code>: Structure used to handle the hessian. The possible values are: <code>HessDense</code>, <code>HessSparse</code>, <code>HessSparseCOO</code>, <code>HessOp</code>. Default: <code>HessOp</code>.</li><li><code>pdata_type::Type{ParamData}</code> Structure used for the preprocessing step. Default: <code>PDataKARC</code>.</li><li><code>robust::Bool</code>: <code>true</code> implements a robust evaluation of the model. Default: <code>true</code>.</li><li><code>verbose::Bool</code>: <code>true</code> prints iteration information. Default: <code>false</code>.</li></ul><p>Additional <code>kwargs</code> are used for stopping criterion, see <code>Stopping.jl</code>.</p><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.stp</code>: stopping object used for the algorithm;</li><li><code>solver.workspace</code>: additional allocations;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p>This implementation uses <code>Stopping.jl</code>. Therefore, it is also possible to used</p><pre><code class="nohighlight hljs">TRARC(stp; kwargs...)</code></pre><p>which returns the <code>stp::NLPStopping</code> updated.</p><p>For advanced usage, first define a <a href="#AdaptiveRegularization.TRARCSolver"><code>TRARCSolver</code></a> to preallocate the  memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">stats = solve!(solver, nlp)
stats = solve!(solver, nlp, stats)</code></pre><p>To choose a particular variant, the keyword arguments <code>hess_type</code> and <code>pdata_type</code> can be used as follows:    <code>TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC)</code> the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.</p><p><strong>References</strong></p><p>This method unifies the implementation of trust-region and adaptive regularization with cubics as described in</p><pre><code class="nohighlight hljs">Dussault, J.-P. (2020).
A unified efficient implementation of trust-region type algorithms for unconstrained optimization.
INFOR: Information Systems and Operational Research, 58(2), 290-309.
10.1080/03155986.2019.1624490</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
stats = TRARC(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = solve!(solver, nlp)</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = GenericExecutionStats(nlp)
stats = solve!(solver, nlp, stats)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/AdaptiveRegularization.jl#L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.TRKsparse-Tuple{Stopping.NLPStopping}" href="#AdaptiveRegularization.TRKsparse-Tuple{Stopping.NLPStopping}"><code>AdaptiveRegularization.TRKsparse</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">TRARC(nlp; kwargs...)</code></pre><p>Compute a local minimum of an unconstrained optimization problem using trust-region (TR)/adaptive regularization with cubics (ARC) methods.</p><p>Some variants of TRARC are already implemented and listed in <code>AdaptiveRegularization.ALL_solvers</code>.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel</code>: the model solved, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments include</p><ul><li><code>TR::ARTrustRegion</code>: structure with trust-region/ARC parameters, see <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl"><code>SolverTools.jl</code></a>. Default: <code>ARTrustRegion(T(10.0))</code>.</li><li><code>hess_type::Type{Hess}</code>: Structure used to handle the hessian. The possible values are: <code>HessDense</code>, <code>HessSparse</code>, <code>HessSparseCOO</code>, <code>HessOp</code>. Default: <code>HessOp</code>.</li><li><code>pdata_type::Type{ParamData}</code> Structure used for the preprocessing step. Default: <code>PDataKARC</code>.</li><li><code>robust::Bool</code>: <code>true</code> implements a robust evaluation of the model. Default: <code>true</code>.</li><li><code>verbose::Bool</code>: <code>true</code> prints iteration information. Default: <code>false</code>.</li></ul><p>Additional <code>kwargs</code> are used for stopping criterion, see <code>Stopping.jl</code>.</p><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.stp</code>: stopping object used for the algorithm;</li><li><code>solver.workspace</code>: additional allocations;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p>This implementation uses <code>Stopping.jl</code>. Therefore, it is also possible to used</p><pre><code class="nohighlight hljs">TRARC(stp; kwargs...)</code></pre><p>which returns the <code>stp::NLPStopping</code> updated.</p><p>For advanced usage, first define a <a href="#AdaptiveRegularization.TRARCSolver"><code>TRARCSolver</code></a> to preallocate the  memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">stats = solve!(solver, nlp)
stats = solve!(solver, nlp, stats)</code></pre><p>To choose a particular variant, the keyword arguments <code>hess_type</code> and <code>pdata_type</code> can be used as follows:    <code>TRARCSolver(nlp; hess_type = ht, pdata_type = PDataKARC)</code> the former specifying how to handle the Hessian information, i.e. matrix-free or sparse matrix, while the latter specifies the type of adaptive approach, e.g. trust-region or ARC.</p><p><strong>References</strong></p><p>This method unifies the implementation of trust-region and adaptive regularization with cubics as described in</p><pre><code class="nohighlight hljs">Dussault, J.-P. (2020).
A unified efficient implementation of trust-region type algorithms for unconstrained optimization.
INFOR: Information Systems and Operational Research, 58(2), 290-309.
10.1080/03155986.2019.1624490</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
stats = TRARC(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = solve!(solver, nlp)</code></pre><pre><code class="language-julia hljs">using AdaptiveRegularization, ADNLPModels, SolverCore
nlp = ADNLPModel(x -&gt; 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]);
solver = TRARCSolver(nlp);
stats = GenericExecutionStats(nlp)
stats = solve!(solver, nlp, stats)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/AdaptiveRegularization.jl#L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.compute_Δq-NTuple{4, Any}" href="#AdaptiveRegularization.compute_Δq-NTuple{4, Any}"><code>AdaptiveRegularization.compute_Δq</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">compute_Δq(Hx, d, ∇f)</code></pre><p>Update <code>Δq = -(∇f + 0.5 * (Hx * d)) ⋅ d</code> in-place.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/main.jl#L63-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.decrease-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData, T, SolverTools.ARTrustRegion}} where T" href="#AdaptiveRegularization.decrease-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData, T, SolverTools.ARTrustRegion}} where T"><code>AdaptiveRegularization.decrease</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">decrease(X::TPData, α::T, TR::ARTrustRegion)</code></pre><p>Return a decreased <code>α</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/utils/increase_decrease.jl#L1-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.hessian!-Tuple{AdaptiveRegularization.AbstractHess, Any, Any}" href="#AdaptiveRegularization.hessian!-Tuple{AdaptiveRegularization.AbstractHess, Any, Any}"><code>AdaptiveRegularization.hessian!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">hessian!(workspace, nlp, x)</code></pre><p>Return the Hessian matrix of <code>nlp</code> at <code>x</code> in-place with memory update of <code>workspace</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/utils/hessian_rep.jl#L98-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.increase-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData, T, SolverTools.ARTrustRegion}} where T" href="#AdaptiveRegularization.increase-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData, T, SolverTools.ARTrustRegion}} where T"><code>AdaptiveRegularization.increase</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">increase(X::TPData, α::T, TR::ARTrustRegion)</code></pre><p>Return an increased <code>α</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/utils/increase_decrease.jl#L10-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.init-Union{Tuple{S}, Tuple{T}, Tuple{Hess}, Tuple{Type{Hess}, NLPModels.AbstractNLPModel{T, S}, Any}} where {Hess&lt;:AdaptiveRegularization.AbstractHess, T, S}" href="#AdaptiveRegularization.init-Union{Tuple{S}, Tuple{T}, Tuple{Hess}, Tuple{Type{Hess}, NLPModels.AbstractNLPModel{T, S}, Any}} where {Hess&lt;:AdaptiveRegularization.AbstractHess, T, S}"><code>AdaptiveRegularization.init</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">init(::Type{Hess}, nlp::AbstractNLPModel{T,S}, n)</code></pre><p>Return the hessian structure <code>Hess</code> and its composite type.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/utils/hessian_rep.jl#L88-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.preprocess!-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData{T}, Vararg{Any, 6}}} where T" href="#AdaptiveRegularization.preprocess!-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData{T}, Vararg{Any, 6}}} where T"><code>AdaptiveRegularization.preprocess!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">preprocess!(PData::TPData, H, g, gNorm2, n1, n2, α)</code></pre><p>Function called in the <code>TRARC</code> algorithm every time a new iterate has been accepted.</p><p><strong>Arguments</strong></p><ul><li><code>PData::TPData</code>: data structure used for preprocessing.</li><li><code>H</code>: current Hessian matrix.</li><li><code>g</code>: current gradient.</li><li><code>gNorm2</code>: 2-norm of the gradient.</li><li><code>n1</code>: Current count on the number of Hessian-vector products.</li><li><code>n2</code>: Maximum number of Hessian-vector products accepted.</li><li><code>α</code>: current value of the TR/ARC parameter.</li></ul><p>It returns <code>PData</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/utils/pdata_struct.jl#L11-L25">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AdaptiveRegularization.solve_model!-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData{T}, Vararg{Any, 6}}} where T" href="#AdaptiveRegularization.solve_model!-Union{Tuple{T}, Tuple{AdaptiveRegularization.TPData{T}, Vararg{Any, 6}}} where T"><code>AdaptiveRegularization.solve_model!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solve_model!(PData::TPData, H, g, gNorm2, n1, n2, α)</code></pre><p>Function called in the <code>TRARC</code> algorithm to solve the subproblem.</p><p><strong>Arguments</strong></p><ul><li><code>PData::TPData</code>: data structure used for preprocessing.</li><li><code>H</code>: current Hessian matrix.</li><li><code>g</code>: current gradient.</li><li><code>gNorm2</code>: 2-norm of the gradient.</li><li><code>n1</code>: Current count on the number of Hessian-vector products.</li><li><code>n2</code>: Maximum number of Hessian-vector products accepted.</li><li><code>α</code>: current value of the TR/ARC parameter.</li></ul><p>It returns a couple <code>(PData.d, PData.λ)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/AdaptiveRegularization.jl/blob/54a765ed26f8f0136cd1559107338d6ceed8f5fa/src/utils/pdata_struct.jl#L30-L45">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../doityourself/">« Do it yourself</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Friday 18 April 2025 15:07">Friday 18 April 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
